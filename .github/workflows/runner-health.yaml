name: Runner Health Check

on:
  schedule:
    # Run every 15 minutes
    - cron: '*/15 * * * *'
  workflow_dispatch:

concurrency:
  group: runner-health-check
  cancel-in-progress: false

permissions:
  contents: read
  actions: read

jobs:
  check-runners:
    name: Check Runner Status
    runs-on: ubuntu-latest
    steps:
      - name: Restore previous state
        id: cache
        uses: actions/cache/restore@v4
        with:
          path: /tmp/runner-health-state
          key: runner-health-state-${{ github.repository }}
          restore-keys: runner-health-state-

      - name: Load previous state
        id: prev
        run: |
          if [ -f /tmp/runner-health-state/state.json ]; then
            cat /tmp/runner-health-state/state.json
            PREV_COUNT=$(jq -r '.unhealthy_count // 0' /tmp/runner-health-state/state.json)
            echo "unhealthy_count=$PREV_COUNT" >> $GITHUB_OUTPUT
            echo "Previous unhealthy count: $PREV_COUNT"
          elif [ -f /tmp/runner-health-state/last_unhealthy_count ]; then
            # Migration from old format
            PREV=$(cat /tmp/runner-health-state/last_unhealthy_count)
            echo "unhealthy_count=$PREV" >> $GITHUB_OUTPUT
            echo "Migrated from old state format, unhealthy count: $PREV"
          else
            echo "unhealthy_count=0" >> $GITHUB_OUTPUT
            echo "No previous state found"
          fi

      - name: Check EC2 instance health
        id: ec2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set +e  # Don't exit on errors

          # Runner instance IDs and names (from docs/gha-runner-setup.md)
          RUNNERS="i-04a15158610df980f:fleet-runner-1 i-0f80c703294413a4c:fleet-runner-2 i-09321b67952c1a208:fleet-runner-3 i-0f2ccaa840ef29450:fleet-runner-4 i-05ecd98e74949dc87:fleet-runner-5"

          # Function to check instance with retries
          check_instance() {
            local id=$1
            local retries=3
            local delay=5
            for i in $(seq 1 $retries); do
              RESULT=$(timeout 10 aws ec2 describe-instance-status --instance-ids "$id" --include-all-instances --query 'InstanceStatuses[0].[InstanceState.Name,InstanceStatus.Status]' --output text 2>/dev/null)
              if [ -n "$RESULT" ] && [ "$RESULT" != "None" ]; then
                echo "$RESULT"
                return 0
              fi
              [ $i -lt $retries ] && sleep $delay
            done
            echo "unknown unknown"
            return 1
          }

          echo "## EC2 Runner Health" >> $GITHUB_STEP_SUMMARY
          echo "| Runner | Instance | State | Health |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|----------|-------|--------|" >> $GITHUB_STEP_SUMMARY

          UNHEALTHY=""
          UNHEALTHY_NAMES=""
          TOTAL=0
          HEALTHY=0
          UNHEALTHY_DETAILS="[]"

          for entry in $RUNNERS; do
            id=$(echo $entry | cut -d: -f1)
            NAME=$(echo $entry | cut -d: -f2)
            TOTAL=$((TOTAL + 1))

            # Get instance state and status with retries
            RESULT=$(check_instance "$id")
            STATE=$(echo $RESULT | awk '{print $1}')
            HEALTH=$(echo $RESULT | awk '{print $2}')

            # Check if healthy (running + ok)
            if [ "$STATE" = "running" ] && [ "$HEALTH" = "ok" ]; then
              HEALTHY=$((HEALTHY + 1))
              echo "| $NAME | $id | âœ… $STATE | $HEALTH |" >> $GITHUB_STEP_SUMMARY
            else
              UNHEALTHY="$UNHEALTHY $id"
              UNHEALTHY_NAMES="$UNHEALTHY_NAMES $NAME"
              UNHEALTHY_DETAILS=$(echo "$UNHEALTHY_DETAILS" | jq --arg id "$id" --arg name "$NAME" --arg state "$STATE" --arg health "$HEALTH" '. + [{"id": $id, "name": $name, "state": $state, "health": $health}]')
              echo "| $NAME | $id | âŒ $STATE | $HEALTH |" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Summary:** $HEALTHY/$TOTAL healthy" >> $GITHUB_STEP_SUMMARY

          # If unhealthy, wait 30s and recheck to avoid transient false positives
          UNHEALTHY_COUNT=$((TOTAL - HEALTHY))
          if [ "$UNHEALTHY_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Confirmation check in 30s...**" >> $GITHUB_STEP_SUMMARY
            sleep 30

            # Recheck only the unhealthy instances
            CONFIRMED_UNHEALTHY=""
            CONFIRMED_NAMES=""
            CONFIRMED_DETAILS="[]"
            for id in $UNHEALTHY; do
              RESULT=$(check_instance "$id")
              STATE=$(echo $RESULT | awk '{print $1}')
              HEALTH=$(echo $RESULT | awk '{print $2}')
              if [ "$STATE" != "running" ] || [ "$HEALTH" != "ok" ]; then
                for entry in $RUNNERS; do
                  eid=$(echo $entry | cut -d: -f1)
                  ename=$(echo $entry | cut -d: -f2)
                  if [ "$eid" = "$id" ]; then
                    CONFIRMED_UNHEALTHY="$CONFIRMED_UNHEALTHY $id"
                    CONFIRMED_NAMES="$CONFIRMED_NAMES $ename"
                    CONFIRMED_DETAILS=$(echo "$CONFIRMED_DETAILS" | jq --arg id "$id" --arg name "$ename" --arg state "$STATE" --arg health "$HEALTH" '. + [{"id": $id, "name": $name, "state": $state, "health": $health}]')
                    break
                  fi
                done
              fi
            done

            UNHEALTHY=$(echo $CONFIRMED_UNHEALTHY | xargs)
            UNHEALTHY_NAMES=$(echo $CONFIRMED_NAMES | xargs)
            UNHEALTHY_COUNT=$(echo $UNHEALTHY | wc -w | tr -d ' ')
            UNHEALTHY_DETAILS="$CONFIRMED_DETAILS"
            HEALTHY=$((TOTAL - UNHEALTHY_COUNT))

            if [ "$UNHEALTHY_COUNT" -eq 0 ]; then
              echo "âœ… **Confirmation: All runners recovered (transient issue)**" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ **Confirmed unhealthy:** $UNHEALTHY_NAMES" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          # Export for subsequent steps
          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "healthy=$HEALTHY" >> $GITHUB_OUTPUT
          echo "unhealthy_count=$UNHEALTHY_COUNT" >> $GITHUB_OUTPUT
          echo "unhealthy_ids=$UNHEALTHY" >> $GITHUB_OUTPUT
          echo "unhealthy_names=$UNHEALTHY_NAMES" >> $GITHUB_OUTPUT
          # Write details to file (too large for output variable)
          echo "$UNHEALTHY_DETAILS" > /tmp/unhealthy_details.json

      - name: Auto-recover unhealthy instances
        id: recover
        if: steps.ec2.outputs.unhealthy_count != '0'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set +e
          MAX_ATTEMPTS=3
          COOLDOWN_SECONDS=3600  # Reset attempt counter after 1 hour

          # Load per-instance recovery state from previous run
          # Format: {"instance_id": {"count": N, "last_attempt": "ISO8601"}, ...}
          # Also handles legacy format: {"instance_id": N}
          if [ -f /tmp/runner-health-state/state.json ]; then
            RAW_ATTEMPTS=$(jq -r '.recovery_attempts // {}' /tmp/runner-health-state/state.json)
          else
            RAW_ATTEMPTS="{}"
          fi

          # Migrate legacy format (plain int) to new format (object with count + timestamp)
          NOW_TS=$(date -u +%s)
          NOW_ISO=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          PREV_ATTEMPTS=$(echo "$RAW_ATTEMPTS" | jq --arg now "$NOW_ISO" '
            to_entries | map(
              if (.value | type) == "number" then
                .value = {"count": .value, "last_attempt": $now}
              else . end
            ) | from_entries
          ')

          # Get list of runners currently executing a job (busy runners)
          BUSY_RUNNERS=$(gh api repos/${{ github.repository }}/actions/runners --paginate --jq '.runners[] | select(.busy == true) | .name' 2>/dev/null || true)
          if [ -n "$BUSY_RUNNERS" ]; then
            echo "Busy runners (will not stop/start):"
            echo "$BUSY_RUNNERS"
          fi

          DETAILS=$(cat /tmp/unhealthy_details.json)
          COUNT=$(echo "$DETAILS" | jq 'length')

          RECOVERED_NAMES=""
          RECOVERED_COUNT=0
          FAILED_NAMES=""
          FAILED_COUNT=0
          SKIPPED_NAMES=""
          SKIPPED_COUNT=0
          UPDATED_ATTEMPTS="$PREV_ATTEMPTS"

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Auto-Recovery" >> $GITHUB_STEP_SUMMARY

          for i in $(seq 0 $((COUNT - 1))); do
            INSTANCE_ID=$(echo "$DETAILS" | jq -r ".[$i].id")
            INSTANCE_NAME=$(echo "$DETAILS" | jq -r ".[$i].name")
            INSTANCE_STATE=$(echo "$DETAILS" | jq -r ".[$i].state")
            INSTANCE_HEALTH=$(echo "$DETAILS" | jq -r ".[$i].health")

            # Get previous attempt count and last attempt time
            ATTEMPTS=$(echo "$PREV_ATTEMPTS" | jq -r --arg id "$INSTANCE_ID" '.[$id].count // 0')
            LAST_ATTEMPT=$(echo "$PREV_ATTEMPTS" | jq -r --arg id "$INSTANCE_ID" '.[$id].last_attempt // empty')
            ELAPSED=0

            # Reset attempts if cooldown has elapsed
            if [ "$ATTEMPTS" -ge "$MAX_ATTEMPTS" ] && [ -n "$LAST_ATTEMPT" ]; then
              LAST_TS=$(date -u -d "$LAST_ATTEMPT" +%s 2>/dev/null || date -u -j -f "%Y-%m-%dT%H:%M:%SZ" "$LAST_ATTEMPT" +%s 2>/dev/null || echo "0")
              ELAPSED=$((NOW_TS - LAST_TS))
              if [ "$ELAPSED" -ge "$COOLDOWN_SECONDS" ]; then
                echo "Cooldown elapsed for $INSTANCE_NAME ($ELAPSED >= $COOLDOWN_SECONDS sec), resetting attempts"
                ATTEMPTS=0
                UPDATED_ATTEMPTS=$(echo "$UPDATED_ATTEMPTS" | jq --arg id "$INSTANCE_ID" 'del(.[$id])')
              fi
            fi

            echo "Processing $INSTANCE_NAME ($INSTANCE_ID): state=$INSTANCE_STATE health=$INSTANCE_HEALTH attempts=$ATTEMPTS/$MAX_ATTEMPTS"

            # Skip if runner is actively executing a job
            if [ -n "$BUSY_RUNNERS" ] && echo "$BUSY_RUNNERS" | grep -qx "$INSTANCE_NAME"; then
              echo "â­ï¸ **$INSTANCE_NAME**: Skipped â€” runner is busy (executing a job)" >> $GITHUB_STEP_SUMMARY
              SKIPPED_NAMES="$SKIPPED_NAMES $INSTANCE_NAME"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              continue
            fi

            # Skip if max attempts reached (cooldown not yet elapsed)
            if [ "$ATTEMPTS" -ge "$MAX_ATTEMPTS" ]; then
              REMAINING=$((COOLDOWN_SECONDS - ELAPSED))
              REMAINING_MIN=$((REMAINING / 60))
              echo "â­ï¸ **$INSTANCE_NAME**: In cooldown â€” $ATTEMPTS attempts exhausted, retrying in ~${REMAINING_MIN}m" >> $GITHUB_STEP_SUMMARY
              SKIPPED_NAMES="$SKIPPED_NAMES $INSTANCE_NAME"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              continue
            fi

            # Skip terminated/shutting-down instances â€” can't recover these
            if [ "$INSTANCE_STATE" = "terminated" ] || [ "$INSTANCE_STATE" = "shutting-down" ]; then
              echo "â­ï¸ **$INSTANCE_NAME**: Skipped â€” instance is $INSTANCE_STATE (needs replacement)" >> $GITHUB_STEP_SUMMARY
              SKIPPED_NAMES="$SKIPPED_NAMES $INSTANCE_NAME"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              continue
            fi

            echo "ðŸ”„ **$INSTANCE_NAME**: Attempting recovery (attempt $((ATTEMPTS + 1))/$MAX_ATTEMPTS)..." >> $GITHUB_STEP_SUMMARY

            # Recovery strategy: stop/start (reboot doesn't fix impaired instances)
            if [ "$INSTANCE_STATE" = "stopped" ]; then
              echo "  Instance already stopped, starting..."
              aws ec2 start-instances --instance-ids "$INSTANCE_ID" > /dev/null 2>&1
            elif [ "$INSTANCE_STATE" = "running" ]; then
              echo "  Stopping instance..."
              aws ec2 stop-instances --instance-ids "$INSTANCE_ID" > /dev/null 2>&1
              echo "  Waiting for instance to stop..."
              aws ec2 wait instance-stopped --instance-ids "$INSTANCE_ID" 2>/dev/null
              echo "  Starting instance..."
              aws ec2 start-instances --instance-ids "$INSTANCE_ID" > /dev/null 2>&1
            elif [ "$INSTANCE_STATE" = "stopping" ]; then
              echo "  Instance is stopping, waiting..."
              aws ec2 wait instance-stopped --instance-ids "$INSTANCE_ID" 2>/dev/null
              echo "  Starting instance..."
              aws ec2 start-instances --instance-ids "$INSTANCE_ID" > /dev/null 2>&1
            else
              echo "  Unexpected state '$INSTANCE_STATE', attempting start..."
              aws ec2 start-instances --instance-ids "$INSTANCE_ID" > /dev/null 2>&1
            fi

            # Wait for instance to be running and pass status checks (up to 10 min)
            echo "  Waiting for instance to pass status checks..."
            if timeout 600 aws ec2 wait instance-status-ok --instance-ids "$INSTANCE_ID" 2>/dev/null; then
              echo "âœ… **$INSTANCE_NAME**: Recovered successfully" >> $GITHUB_STEP_SUMMARY
              RECOVERED_NAMES="$RECOVERED_NAMES $INSTANCE_NAME"
              RECOVERED_COUNT=$((RECOVERED_COUNT + 1))
              # Clear attempt tracking on success
              UPDATED_ATTEMPTS=$(echo "$UPDATED_ATTEMPTS" | jq --arg id "$INSTANCE_ID" 'del(.[$id])')
            else
              NEW_COUNT=$((ATTEMPTS + 1))
              echo "âŒ **$INSTANCE_NAME**: Recovery failed (attempt $NEW_COUNT/$MAX_ATTEMPTS)" >> $GITHUB_STEP_SUMMARY
              FAILED_NAMES="$FAILED_NAMES $INSTANCE_NAME"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              # Record attempt count and timestamp
              UPDATED_ATTEMPTS=$(echo "$UPDATED_ATTEMPTS" | jq --arg id "$INSTANCE_ID" --argjson n "$NEW_COUNT" --arg ts "$NOW_ISO" '.[$id] = {"count": $n, "last_attempt": $ts}')
            fi
          done

          RECOVERED_NAMES=$(echo $RECOVERED_NAMES | xargs)
          FAILED_NAMES=$(echo $FAILED_NAMES | xargs)
          SKIPPED_NAMES=$(echo $SKIPPED_NAMES | xargs)
          STILL_UNHEALTHY=$((FAILED_COUNT + SKIPPED_COUNT))

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Recovery summary:** $RECOVERED_COUNT recovered, $FAILED_COUNT failed, $SKIPPED_COUNT skipped" >> $GITHUB_STEP_SUMMARY

          echo "recovered_count=$RECOVERED_COUNT" >> $GITHUB_OUTPUT
          echo "recovered_names=$RECOVERED_NAMES" >> $GITHUB_OUTPUT
          echo "failed_count=$FAILED_COUNT" >> $GITHUB_OUTPUT
          echo "failed_names=$FAILED_NAMES" >> $GITHUB_OUTPUT
          echo "skipped_count=$SKIPPED_COUNT" >> $GITHUB_OUTPUT
          echo "skipped_names=$SKIPPED_NAMES" >> $GITHUB_OUTPUT
          echo "still_unhealthy=$STILL_UNHEALTHY" >> $GITHUB_OUTPUT

          # Save updated attempts for state step
          echo "$UPDATED_ATTEMPTS" > /tmp/updated_recovery_attempts.json

      - name: Check queued jobs
        id: queue
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          QUEUED=$(gh run list --repo ${{ github.repository }} --status queued --json databaseId --jq 'length' 2>/dev/null || echo "0")
          IN_PROGRESS=$(gh run list --repo ${{ github.repository }} --status in_progress --json databaseId --jq 'length' 2>/dev/null || echo "0")

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Jobs:** $QUEUED queued, $IN_PROGRESS in progress" >> $GITHUB_STEP_SUMMARY

          echo "queued=$QUEUED" >> $GITHUB_OUTPUT
          echo "in_progress=$IN_PROGRESS" >> $GITHUB_OUTPUT

      - name: Check if alert needed
        id: alert_check
        if: steps.recover.outputs.still_unhealthy != '' && steps.recover.outputs.still_unhealthy != '0'
        run: |
          # Only send Slack alert for NEW unhealthy runners not already alerted
          if [ -f /tmp/runner-health-state/state.json ]; then
            PREV_ALERTED=$(jq -r '.alerted_instances // [] | .[]' /tmp/runner-health-state/state.json)
          else
            PREV_ALERTED=""
          fi

          FAILED="${{ steps.recover.outputs.failed_names }}"
          SKIPPED="${{ steps.recover.outputs.skipped_names }}"
          ALL_UNHEALTHY="$FAILED $SKIPPED"

          NEW_ALERTS=""
          for name in $ALL_UNHEALTHY; do
            if ! echo "$PREV_ALERTED" | grep -qx "$name"; then
              NEW_ALERTS="$NEW_ALERTS $name"
            fi
          done
          NEW_ALERTS=$(echo $NEW_ALERTS | xargs)

          if [ -n "$NEW_ALERTS" ]; then
            echo "should_alert=true" >> $GITHUB_OUTPUT
            echo "new_unhealthy=$NEW_ALERTS" >> $GITHUB_OUTPUT
            echo "New unhealthy runners to alert on: $NEW_ALERTS"
          else
            echo "should_alert=false" >> $GITHUB_OUTPUT
            echo "All unhealthy runners already alerted, skipping Slack"
          fi

          # Update alerted list (will be persisted in save step)
          ALL_ALERTED="$PREV_ALERTED"
          for name in $ALL_UNHEALTHY; do
            if ! echo "$ALL_ALERTED" | grep -qx "$name"; then
              ALL_ALERTED="$ALL_ALERTED
          $name"
            fi
          done
          # Write as JSON array for save step
          echo "$ALL_ALERTED" | grep -v '^$' | jq -R . | jq -s . > /tmp/updated_alerted.json

      - name: Save current state
        if: always()
        run: |
          mkdir -p /tmp/runner-health-state

          # Load recovery attempts (from auto-recover step, or previous state)
          if [ -f /tmp/updated_recovery_attempts.json ]; then
            ATTEMPTS=$(cat /tmp/updated_recovery_attempts.json)
          elif [ -f /tmp/runner-health-state/state.json ]; then
            ATTEMPTS=$(jq -r '.recovery_attempts // {}' /tmp/runner-health-state/state.json)
          else
            ATTEMPTS="{}"
          fi

          # Clean up recovery_attempts for instances that are now healthy
          UNHEALTHY_IDS="${{ steps.ec2.outputs.unhealthy_ids }}"
          ATTEMPTS=$(echo "$ATTEMPTS" | jq --arg ids "$UNHEALTHY_IDS" '
            ($ids | split(" ") | map(select(. != ""))) as $unhealthy |
            to_entries | map(select(.key as $k | $unhealthy | index($k))) | from_entries
          ')

          # Determine final unhealthy count (after recovery)
          STILL_UNHEALTHY="${{ steps.recover.outputs.still_unhealthy }}"
          if [ -z "$STILL_UNHEALTHY" ]; then
            STILL_UNHEALTHY="${{ steps.ec2.outputs.unhealthy_count }}"
          fi

          # Load alerted list (from alert_check step, or previous state, pruned to still-unhealthy)
          if [ -f /tmp/updated_alerted.json ]; then
            ALERTED=$(cat /tmp/updated_alerted.json)
          elif [ -f /tmp/runner-health-state/state.json ]; then
            ALERTED=$(jq -r '.alerted_instances // []' /tmp/runner-health-state/state.json)
          else
            ALERTED="[]"
          fi
          # Prune alerted list to only currently unhealthy runners
          UNHEALTHY_NAMES="${{ steps.ec2.outputs.unhealthy_names }}"
          ALERTED=$(echo "$ALERTED" | jq --arg names "$UNHEALTHY_NAMES" '
            ($names | split(" ") | map(select(. != ""))) as $current |
            map(select(. as $a | $current | index($a))) |
            unique
          ')

          jq -n \
            --argjson attempts "$ATTEMPTS" \
            --argjson count "${STILL_UNHEALTHY:-0}" \
            --argjson alerted "$ALERTED" \
            '{unhealthy_count: $count, recovery_attempts: $attempts, alerted_instances: $alerted, updated_at: now | todate}' \
            > /tmp/runner-health-state/state.json

          # Remove old format file if it exists
          rm -f /tmp/runner-health-state/last_unhealthy_count

          echo "Saved state:"
          cat /tmp/runner-health-state/state.json

      - name: Update cache
        if: always()
        uses: actions/cache/save@v4
        with:
          path: /tmp/runner-health-state
          key: runner-health-state-${{ github.repository }}-${{ github.run_id }}

      - name: Alert - auto-recovery succeeded
        if: steps.recover.outputs.recovered_count != '' && steps.recover.outputs.recovered_count != '0' && steps.recover.outputs.still_unhealthy == '0'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          method: chat.postMessage
          token: ${{ secrets.SLACK_BOT_TOKEN }}
          payload: |
            {
              "channel": "#fleet-training-runs",
              "text": "ðŸ”§ Runners Auto-Recovered",
              "blocks": [
                {
                  "type": "header",
                  "text": { "type": "plain_text", "text": "ðŸ”§ Runners Auto-Recovered" }
                },
                {
                  "type": "section",
                  "fields": [
                    { "type": "mrkdwn", "text": "*Recovered:*\n${{ steps.recover.outputs.recovered_names }}" },
                    { "type": "mrkdwn", "text": "*Method:*\nEC2 stop/start" }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "No action needed. | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>" }
                  ]
                }
              ]
            }

      - name: Alert - manual intervention needed
        if: steps.alert_check.outputs.should_alert == 'true'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          method: chat.postMessage
          token: ${{ secrets.SLACK_BOT_TOKEN }}
          payload: |
            {
              "channel": "#fleet-training-runs",
              "text": "ðŸš¨ Runner Recovery Failed â€” Manual Intervention Needed",
              "blocks": [
                {
                  "type": "header",
                  "text": { "type": "plain_text", "text": "ðŸš¨ Manual Intervention Needed" }
                },
                {
                  "type": "section",
                  "fields": [
                    { "type": "mrkdwn", "text": "*Still unhealthy:*\n${{ steps.recover.outputs.failed_names }} ${{ steps.recover.outputs.skipped_names }}" },
                    { "type": "mrkdwn", "text": "*Auto-recovered:*\n${{ steps.recover.outputs.recovered_names || 'none' }}" }
                  ]
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "Auto-recovery failed. Will retry after 1h cooldown.\nManual fix: `aws ec2 stop-instances --instance-ids <ID>` then `aws ec2 start-instances --instance-ids <ID>`\nOr launch a replacement runner per `docs/gha-runner-setup.md`."
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "<https://github.com/${{ github.repository }}/settings/actions/runners|Manage Runners> | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>" }
                  ]
                }
              ]
            }

      - name: Alert - all runners healthy (recovered from previous issue)
        if: steps.ec2.outputs.unhealthy_count == '0' && steps.prev.outputs.unhealthy_count != '0'
        uses: slackapi/slack-github-action@v2.0.0
        with:
          method: chat.postMessage
          token: ${{ secrets.SLACK_BOT_TOKEN }}
          payload: |
            {
              "channel": "#fleet-training-runs",
              "text": "âœ… All Runners Healthy",
              "blocks": [
                {
                  "type": "header",
                  "text": { "type": "plain_text", "text": "âœ… All Runners Healthy" }
                },
                {
                  "type": "section",
                  "text": { "type": "mrkdwn", "text": "*Status:* ${{ steps.ec2.outputs.healthy }}/${{ steps.ec2.outputs.total }} runners online" }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>" }
                  ]
                }
              ]
            }

      - name: Report status
        if: steps.recover.outputs.still_unhealthy != '' && steps.recover.outputs.still_unhealthy != '0'
        run: |
          echo "::warning::${{ steps.recover.outputs.still_unhealthy }} runner(s) still unhealthy: ${{ steps.recover.outputs.failed_names }} ${{ steps.recover.outputs.skipped_names }} (will auto-retry after cooldown)"
