import torch
from tqdm import tqdm
from typing import Dict, List, Any
from pathlib import Path
from loguru import logger
from collections import defaultdict

from skyrl_train.utils import Timer

from skyrl_train.generators.utils import (
    concatenate_generator_outputs,
    prepare_generator_input,
)
from skyrl_train.generators.base import (
    GeneratorOutput,
    GeneratorInterface,
)
from skyrl_train.utils.trainer_utils import (
    calculate_per_dataset_metrics,
    dump_per_dataset_eval_results,
    validate_generator_output,
)
from skyrl_train.metrics import compute_reward_metrics
from skyrl_train.inference_engines.utils import get_sampling_params_for_backend
from skyrl_train.utils.logging_utils import log_example

from omegaconf import DictConfig
from torchdata.stateful_dataloader import StatefulDataLoader
from transformers import AutoTokenizer


@torch.no_grad()
async def evaluate(
    eval_dataloader: StatefulDataLoader,
    generator: GeneratorInterface,
    cfg: DictConfig,
    global_step: int | None,
    tokenizer: AutoTokenizer,
) -> Dict[str, float]:
    """Runs generation and evaluation of trajectories.

    Args:
        eval_dataloader (StatefulDataLoader): dataloader of the eval dataset
        generator (GeneratorInterface): generator to use
        cfg (DictConfig): config
        global_step (int | None): current global step, or
            `None` to indicate a non-training context (e.g., eval-only)
        tokenizer (AutoTokenizer): tokenizer to use

    Returns:
        Dict[str, float]: evaluation metrics
    """

    # 1. Get all generator outputs
    generator_outputs: List[GeneratorOutput] = []
    concat_all_envs: List[str] = []
    concat_env_extras: List[Dict[str, Any]] = []
    concat_uids: List[str] = []
    sampling_params = cfg.generator.eval_sampling_params
    pbar = tqdm(total=len(eval_dataloader), initial=0, desc="Evaluation Progress")
    for _, prompts in enumerate(eval_dataloader):
        pbar.update(1)
        generator_input, uids = prepare_generator_input(
            prompts,
            cfg.generator.eval_n_samples_per_prompt,
            get_sampling_params_for_backend(cfg.generator.backend, sampling_params),
            cfg.environment.env_class,
            "eval",
            global_step,
        )
        generator_output: GeneratorOutput = await generator.generate(generator_input)
        validate_generator_output(len(generator_input["prompts"]), generator_output)
        generator_outputs.append(generator_output)
        concat_all_envs.extend(generator_input["env_classes"])
        concat_env_extras.extend(generator_input["env_extras"])
        concat_uids.extend(uids)
    concat_generator_outputs: GeneratorOutput = concatenate_generator_outputs(generator_outputs)

    # Extract data_sources from env_extras
    concat_data_sources = [env_extra.get("data_source") for env_extra in concat_env_extras]
    vis = tokenizer.decode(generator_output["response_ids"][0])
    log_example(
        logger,
        prompt=generator_input["prompts"][0],
        response=vis,
        reward=generator_output["rewards"][0],
    )

    # Log eval stop_reason breakdown by environment
    _log_eval_stop_reasons(concat_generator_outputs, concat_data_sources, global_step)

    # 2. Group data by data source and calculate per-dataset metrics
    eval_metrics = calculate_per_dataset_metrics(
        concat_generator_outputs, concat_uids, concat_data_sources, cfg.generator.eval_n_samples_per_prompt
    )

    # 3. Calculate overall metrics across all datasets using shared module
    overall_metrics = compute_reward_metrics(
        rewards=concat_generator_outputs["rewards"],
        uids=concat_uids,
        n_samples_per_prompt=cfg.generator.eval_n_samples_per_prompt,
    )
    eval_metrics.update(
        {
            f"eval/all/pass_at_{cfg.generator.eval_n_samples_per_prompt}": overall_metrics[
                f"pass_at_{cfg.generator.eval_n_samples_per_prompt}"
            ],
            "eval/all/variance_per_prompt": overall_metrics["variance_per_prompt"],
            "eval/all/mean_positive_reward": overall_metrics["mean_positive_reward"],
        }
    )

    # 4. Prepare dumping data and upload to S3
    if cfg.trainer.dump_eval_results:
        with Timer("dump_eval_results"):
            data_save_dir = (
                Path(cfg.trainer.export_path)
                / "dumped_evals"
                / ("eval_only" if global_step is None else f"global_step_{global_step}_evals")
            )
            data_save_dir.mkdir(parents=True, exist_ok=True)
            dump_per_dataset_eval_results(
                data_save_dir,
                tokenizer,
                concat_generator_outputs,
                concat_data_sources,
                concat_all_envs,
                concat_env_extras,
                eval_metrics,
            )

            # Upload to S3 if credentials are available
            try:
                from integrations.fleet.s3_checkpoints import upload_eval_results_to_s3

                run_name = getattr(cfg.trainer, "run_name", None)
                if run_name:
                    upload_eval_results_to_s3(
                        local_dir=str(data_save_dir),
                        run_name=run_name,
                        global_step=global_step,
                        delete_local=False,  # Keep local copy
                    )
            except ImportError:
                pass  # S3 upload not available
            except Exception as e:
                logger.warning(f"Failed to upload eval results to S3: {e}")

    return eval_metrics


@torch.no_grad()
async def evaluate_step_wise(
    eval_dataloader: StatefulDataLoader,
    generator: GeneratorInterface,
    cfg: DictConfig,
    global_step: int | None,
    tokenizer: AutoTokenizer,
) -> Dict[str, float]:
    """Runs generation and evaluation of trajectories for step-wise training.

    Currently assumes that the rewards are assigned to the last step of each trajectory.

    Args:
        eval_dataloader (StatefulDataLoader): dataloader of the eval dataset
        generator (GeneratorInterface): generator to use
        cfg (DictConfig): config
        global_step (int | None): current global step, or
            `None` to indicate a non-training context (e.g., eval-only)
        tokenizer (AutoTokenizer): tokenizer to use

    Returns:
        Dict[str, float]: evaluation metrics
    """

    # 1. Get all generator outputs
    generator_outputs: List[GeneratorOutput] = []
    concat_all_envs: List[str] = []
    concat_env_extras: List[Dict[str, Any]] = []
    concat_uids: List[str] = []
    sampling_params = cfg.generator.eval_sampling_params
    pbar = tqdm(total=len(eval_dataloader), initial=0, desc="Evaluation Progress")
    for _, prompts in enumerate(eval_dataloader):
        pbar.update(1)
        generator_input, uids = prepare_generator_input(
            prompts,
            cfg.generator.eval_n_samples_per_prompt,
            get_sampling_params_for_backend(cfg.generator.backend, sampling_params),
            cfg.environment.env_class,
            "eval",
            global_step,
        )
        generator_output: GeneratorOutput = await generator.generate(generator_input)
        traj_id_to_input = {
            traj_id.instance_id: {"env_class": env_class, "env_extras": env_extra}
            for traj_id, env_class, env_extra in zip(
                generator_input["trajectory_ids"], generator_input["env_classes"], generator_input["env_extras"]
            )
        }
        for traj_id in generator_output["trajectory_ids"]:
            assert traj_id.instance_id in traj_id_to_input, f"Trajectory ID {traj_id.instance_id} not found in input"
            concat_all_envs.append(traj_id_to_input[traj_id.instance_id]["env_class"])
            concat_env_extras.append(traj_id_to_input[traj_id.instance_id]["env_extras"])
            concat_uids.append(traj_id.instance_id)
        # validate_generator_output(generator_input, generator_output)
        generator_outputs.append(generator_output)
    concat_generator_outputs: GeneratorOutput = concatenate_generator_outputs(generator_outputs)

    # Extract data_sources from env_extras
    concat_data_sources = [env_extra.get("data_source") for env_extra in concat_env_extras]
    vis = tokenizer.decode(generator_output["response_ids"][0])
    logger.info(f"Eval output example: {vis}")

    # Log eval stop_reason breakdown by environment
    _log_eval_stop_reasons(concat_generator_outputs, concat_data_sources, global_step)

    # Only use the final step metrics
    generator_output_last_step = defaultdict(list)
    is_last_step_mask = concat_generator_outputs["is_last_step"]
    for key in concat_generator_outputs:
        if isinstance(concat_generator_outputs[key], list):
            assert len(concat_generator_outputs[key]) == len(
                is_last_step_mask
            ), f"Length mismatch: {len(concat_generator_outputs[key])} != {len(is_last_step_mask)} for key {key}"
            generator_output_last_step[key] = [
                val for val, is_last_step in zip(concat_generator_outputs[key], is_last_step_mask) if is_last_step
            ]
    uids_last_step = [uid for uid, is_last_step in zip(concat_uids, is_last_step_mask) if is_last_step]
    data_sources_last_step = [
        data_source for data_source, is_last_step in zip(concat_data_sources, is_last_step_mask) if is_last_step
    ]

    # 2. Group data by data source and calculate per-dataset metrics
    eval_metrics = calculate_per_dataset_metrics(
        generator_output_last_step, uids_last_step, data_sources_last_step, cfg.generator.eval_n_samples_per_prompt
    )
    # 3. Calculate overall metrics across all datasets using shared module
    overall_metrics = compute_reward_metrics(
        rewards=generator_output_last_step["rewards"],
        uids=uids_last_step,
        n_samples_per_prompt=cfg.generator.eval_n_samples_per_prompt,
    )
    eval_metrics.update(
        {
            f"eval/all/pass_at_{cfg.generator.eval_n_samples_per_prompt}": overall_metrics[
                f"pass_at_{cfg.generator.eval_n_samples_per_prompt}"
            ],
            "eval/all/variance_per_prompt": overall_metrics["variance_per_prompt"],
            "eval/all/mean_positive_reward": overall_metrics["mean_positive_reward"],
        }
    )

    # 4. Prepare dumping data and upload to S3
    if cfg.trainer.dump_eval_results:
        with Timer("dump_eval_results"):
            data_save_dir = (
                Path(cfg.trainer.export_path)
                / "dumped_evals"
                / ("eval_only" if global_step is None else f"global_step_{global_step}_evals")
            )
            data_save_dir.mkdir(parents=True, exist_ok=True)
            dump_per_dataset_eval_results(
                data_save_dir,
                tokenizer,
                concat_generator_outputs,
                concat_data_sources,
                concat_all_envs,
                concat_env_extras,
                eval_metrics,
            )

            # Upload to S3 if credentials are available
            try:
                from integrations.fleet.s3_checkpoints import upload_eval_results_to_s3

                run_name = getattr(cfg.trainer, "run_name", None)
                if run_name:
                    upload_eval_results_to_s3(
                        local_dir=str(data_save_dir),
                        run_name=run_name,
                        global_step=global_step,
                        delete_local=False,  # Keep local copy
                    )
            except ImportError:
                pass  # S3 upload not available
            except Exception as e:
                logger.warning(f"Failed to upload eval results to S3: {e}")

    return eval_metrics


def _log_eval_stop_reasons(
    generator_output: GeneratorOutput,
    data_sources: List[str],
    global_step: int | None,
) -> None:
    """Log stop_reason breakdown by environment during eval.

    Outputs a table to logs showing per-env counts of each stop_reason
    (stop, timeout, batch_timeout, env_init_failed, etc.) so failures
    are visible in GHA logs without needing WandB.
    """
    stop_reasons = generator_output.get("stop_reasons")
    if not stop_reasons:
        return

    # Count (data_source, stop_reason) pairs
    env_stop_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    for ds, sr in zip(data_sources, stop_reasons):
        env_stop_counts[ds or "unknown"][sr] += 1

    # Collect all stop_reasons seen
    all_reasons = sorted({sr for counts in env_stop_counts.values() for sr in counts})

    # Log summary
    step_label = f"step {global_step}" if global_step is not None else "eval_only"
    total = len(stop_reasons)
    non_stop = sum(1 for sr in stop_reasons if sr != "stop")

    if non_stop > 0:
        logger.warning(
            f"Eval stop_reason summary ({step_label}): {non_stop}/{total} "
            f"non-stop trajectories ({100 * non_stop / total:.1f}%)"
        )
    else:
        logger.info(f"Eval stop_reason summary ({step_label}): all {total} trajectories completed normally")

    # Build per-environment table
    header = f"  {'Environment':<30} {'total':>6}"
    for reason in all_reasons:
        header += f" {reason:>16}"
    logger.info(header)
    logger.info("  " + "-" * (len(header) - 2))

    for env in sorted(env_stop_counts):
        counts = env_stop_counts[env]
        env_total = sum(counts.values())
        row = f"  {env:<30} {env_total:>6}"
        for reason in all_reasons:
            count = counts.get(reason, 0)
            row += f" {count:>16}"
        logger.info(row)
