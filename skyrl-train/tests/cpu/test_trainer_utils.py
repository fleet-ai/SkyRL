"""
uv run --isolated --extra dev pytest tests/cpu/test_trainer_utils.py
"""

from skyrl_train.utils.trainer_utils import (
    run_on_each_node,
    cleanup_old_checkpoints,
    validate_consistency_for_latest_checkpoint,
    sanitize_data_source,
    calculate_per_dataset_metrics,
    dump_per_dataset_eval_results,
    handle_dynamic_sampling,
    handle_replace_sampling,
    handle_filter_sampling,
    filter_generator_output,
    zero_variance_filter,
    validate_generator_output,
    build_dataloader,
    HybridEnvSampler,
)
from skyrl_train.generators.base import GeneratorInput, GeneratorOutput
from typing import Union
import ray
import os
import tempfile
import pytest
import re
import torch

from unittest.mock import Mock, patch, mock_open
import json
from tests.cpu.util import example_dummy_config

BasicType = Union[int, float, str, bool, type(None)]


@pytest.fixture
def dummy_config():
    return example_dummy_config()


def test_run_on_node_local_rank_0():
    def fn(x):
        return x + 1

    all_nodes = [node for node in ray.nodes() if node.get("CPU", 0) > 0]
    # repeat the node ids 4 times to test that the function is called only once per node
    node_ids = [all_nodes[i]["NodeID"] for i in range(len(all_nodes))] * 4
    ret = run_on_each_node(node_ids, fn, 1)
    assert ret == [2] * len(all_nodes)


def setup_mock_ckpts(tmpdir, checkpoint_steps):
    """
    Sets up dummy checkpoint directories.
    """
    # Create dummy checkpoint directories
    for step in checkpoint_steps:
        os.makedirs(os.path.join(tmpdir, f"global_step_{step}"))
    return


def test_cleanup_old_checkpoints():
    """
    Verify that _cleanup_old_checkpoints correctly removes old checkpoints
    while keeping the most recent ones.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 10, 11]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # 2. Execute
        cleanup_old_checkpoints(tmpdir, max_checkpoints=2)

        # 3. Verify
        remaining_dirs = sorted(os.listdir(tmpdir))
        expected_remaining = ["global_step_10", "global_step_11"]

        assert len(remaining_dirs) == 2, "Incorrect number of checkpoints remaining"
        assert remaining_dirs == expected_remaining, "Did not keep the correct (most recent) checkpoints"

    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 10, 11]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # 2. Execute
        cleanup_old_checkpoints(tmpdir, max_checkpoints=0)

        # 3. Verify
        remaining_dirs = sorted(os.listdir(tmpdir))

        assert len(remaining_dirs) == 0, "Cleanup should have removed all checkpoints"

    # Test cleanup with `current_global_step` less than the highest global step in the folder
    # This means that the folder contains checkpoints from a previous run.
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 10, 11]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # 2. Execute
        cleanup_old_checkpoints(tmpdir, max_checkpoints=4)

        remaining_dirs = sorted(os.listdir(tmpdir))
        assert len(remaining_dirs) == 4, "Cleanup should not have removed any checkpoints"


def test_cleanup_does_not_run_when_not_needed():
    """
    Verify that cleanup does not remove any checkpoints if the total number
    is less than or equal to max_checkpoints.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 3, 4]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # 2. Execute
        cleanup_old_checkpoints(tmpdir, max_checkpoints=5)

        # 3. Verify
        remaining_dirs = sorted(os.listdir(tmpdir))
        assert len(remaining_dirs) == 4, "Cleanup should not have removed any checkpoints"


def test_cleanup_with_negative_max_checkpoints():
    """
    Verify that cleanup is disabled when max_checkpoints is -1
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 3, 4, 5]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # 2. Execute
        cleanup_old_checkpoints(tmpdir, max_checkpoints=-1)

        # 3. Verify
        remaining_dirs = sorted(os.listdir(tmpdir))
        assert len(remaining_dirs) == 5, "Cleanup should be disabled when max_checkpoints is -1"


def test_validate_consistency_for_latest_checkpoint():
    """
    Verify that `validate_consistency_for_latest_checkpoint` correctly validates the checkpoint folder.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 3, 4, 5]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        latest_ckpt_file = os.path.join(tmpdir, "latest_ckpt_global_step.txt")
        with open(latest_ckpt_file, "w") as f:
            f.write("5")

        latest_ckpt_path = os.path.join(tmpdir, "global_step_5")
        ckpt_iteration = 5

        # 2. Execute
        validate_consistency_for_latest_checkpoint(
            tmpdir, ckpt_iteration, latest_ckpt_path, latest_ckpt_file, save_interval=1
        )


def test_validate_consistency_for_latest_checkpoint_with_inconsistent_folder():
    """
    Verify that `validate_consistency_for_latest_checkpoint` correctly validates the checkpoint folder.
    """
    # Example 1: `latest_ckpt_global_step.txt` points to a lower global step than the highest global step in the folder
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 2, 3, 4, 5]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # change the latest checkpoint file to point to a lower global step
        latest_ckpt_file = os.path.join(tmpdir, "latest_ckpt_global_step.txt")
        with open(latest_ckpt_file, "w") as f:
            f.write("3")

        latest_ckpt_path = os.path.join(tmpdir, "global_step_3")
        ckpt_iteration = 3
        save_interval = 1

        # 2. Execute
        with pytest.raises(ValueError, match="Inconsistent checkpoint folder"):
            validate_consistency_for_latest_checkpoint(
                tmpdir, ckpt_iteration, latest_ckpt_path, latest_ckpt_file, save_interval=save_interval
            )

    # Example 2: `latest_ckpt_global_step.txt` points to a lower global step but it's within the save interval
    with tempfile.TemporaryDirectory() as tmpdir:
        # 1. Setup
        checkpoint_steps = [1, 3, 5]
        setup_mock_ckpts(tmpdir, checkpoint_steps=checkpoint_steps)

        # change the latest checkpoint file to point to a lower global step
        latest_ckpt_file = os.path.join(tmpdir, "latest_ckpt_global_step.txt")
        with open(latest_ckpt_file, "w") as f:
            f.write("3")

        save_interval = 2
        latest_ckpt_path = os.path.join(tmpdir, "global_step_3")
        ckpt_iteration = 3

        # 2. Execute
        validate_consistency_for_latest_checkpoint(
            tmpdir, ckpt_iteration, latest_ckpt_path, latest_ckpt_file, save_interval=save_interval
        )


def test_sanitize_data_source_none():
    """Test sanitize_data_source with None input."""
    result = sanitize_data_source(None)
    assert result == "unknown"


def test_sanitize_data_source_slash_replacement():
    """Test sanitize_data_source replaces slashes with underscores."""
    result = sanitize_data_source("dataset/with/slashes")
    assert result == "dataset_with_slashes"


def test_sanitize_data_source_normal_string():
    """Test sanitize_data_source with normal string."""
    result = sanitize_data_source("normal_dataset")
    assert result == "normal_dataset"


def test_calculate_per_dataset_metrics_single_source():
    """Test calculate_per_dataset_metrics with single data source."""
    # Create test data
    generator_outputs = {
        "rewards": [0.5, 0.7, 0.9],
        "prompt_token_ids": [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
        "response_ids": [[10, 11], [12, 13], [14, 15]],
    }
    uids = ["uid1", "uid2", "uid3"]
    data_sources = ["dataset1", "dataset1", "dataset1"]

    result = calculate_per_dataset_metrics(generator_outputs, uids, data_sources, 2)

    # Verify results - actual computed values
    # Pass@N: all rewards > 0, all unique uids, so 3/3 = 1.0
    # variance_per_prompt: 0.0 (single sample per uid)
    # mean_positive_reward: (0.5 + 0.7 + 0.9) / 3 = 0.7
    assert "eval/dataset1/pass_at_2" in result
    assert "eval/dataset1/variance_per_prompt" in result
    assert "eval/dataset1/mean_positive_reward" in result
    assert result["eval/dataset1/pass_at_2"] == 1.0
    assert result["eval/dataset1/variance_per_prompt"] == 0.0  # single sample per uid
    assert result["eval/dataset1/mean_positive_reward"] == pytest.approx(0.7)


def test_calculate_per_dataset_metrics_multiple_sources():
    """Test calculate_per_dataset_metrics with multiple data sources including None."""
    # Create test data with mixed sources
    generator_outputs = {
        "rewards": [0.5, 0.7, 0.9, 0.4],
        "prompt_token_ids": [[1, 2], [3, 4], [5, 6], [7, 8]],
        "response_ids": [[10, 11], [12, 13], [14, 15], [16, 17]],
    }
    uids = ["uid1", "uid2", "uid3", "uid4"]
    data_sources = ["dataset1", None, "dataset1", None]

    result = calculate_per_dataset_metrics(generator_outputs, uids, data_sources, 2)

    # Verify results for both datasets - actual computed values
    # dataset1: indices 0, 2 -> rewards [0.5, 0.9] -> mean_positive = 0.7, pass@n = 2/2 = 1.0
    # unknown (None): indices 1, 3 -> rewards [0.7, 0.4] -> mean_positive = 0.55, pass@n = 2/2 = 1.0
    assert "eval/dataset1/pass_at_2" in result
    assert "eval/dataset1/variance_per_prompt" in result
    assert "eval/dataset1/mean_positive_reward" in result
    assert "eval/unknown/pass_at_2" in result
    assert "eval/unknown/variance_per_prompt" in result
    assert "eval/unknown/mean_positive_reward" in result

    assert result["eval/dataset1/pass_at_2"] == 1.0
    assert result["eval/dataset1/variance_per_prompt"] == 0.0  # single sample per uid
    assert result["eval/dataset1/mean_positive_reward"] == pytest.approx(0.7)
    assert result["eval/unknown/pass_at_2"] == 1.0
    assert result["eval/unknown/variance_per_prompt"] == 0.0  # single sample per uid
    assert result["eval/unknown/mean_positive_reward"] == pytest.approx(0.55)


@patch("builtins.open", new_callable=mock_open)
def test_dump_per_dataset_eval_results_comprehensive(mock_file):
    """Test dump_per_dataset_eval_results comprehensive functionality."""
    # Mock dump directory path
    mock_dump_dir = Mock()
    mock_dump_dir.__truediv__ = Mock(side_effect=lambda x: f"mock_path/{x}")

    # Mock tokenizer
    mock_tokenizer = Mock()
    mock_tokenizer.decode.side_effect = lambda x: f"decoded_{x}"

    # Create test data
    generator_outputs = {
        "prompt_token_ids": [[1, 2], [3, 4], [5, 6]],
        "response_ids": [[10, 11], [12, 13], [14, 15]],
        "rewards": [0.5, 0.7, 0.9],
        "stop_reasons": ["stop1", "stop2", "stop3"],
    }
    data_sources = ["dataset1", None, "dataset1"]
    all_envs = ["env1", "env2", "env3"]
    env_extras = [{"extra1": "val1"}, {"extra2": "val2"}, {"extra3": "val3"}]
    eval_metrics = {"eval/dataset1/avg_score": 0.8, "eval/unknown/avg_score": 0.6}

    # Call the function
    dump_per_dataset_eval_results(
        mock_dump_dir, mock_tokenizer, generator_outputs, data_sources, all_envs, env_extras, eval_metrics
    )

    # Verify tokenizer was called for decoding
    assert mock_tokenizer.decode.call_count == 6  # 3 prompts + 3 responses

    # Verify files were opened (2 per-dataset files + 1 aggregated file)
    assert mock_file.call_count == 3

    # Verify file writes occurred
    handle = mock_file.return_value
    assert handle.write.call_count > 0

    # Verify JSON structure by checking some write calls contain expected data
    write_calls = [call[0][0] for call in handle.write.call_args_list]
    json_writes = [call for call in write_calls if call.strip() and not call.startswith("Dumped")]

    # At least one JSON line should contain our test data
    assert len(json_writes) > 0

    # Parse one of the JSON writes to verify structure
    for write_call in json_writes:
        try:
            data = json.loads(write_call.strip())
            if "input_prompt" in data:
                # This is a per-dataset entry
                assert "output_response" in data
                assert "score" in data
                assert "data_source" in data
                break
        except json.JSONDecodeError:
            continue


def test_handle_dynamic_sampling_null_strategy():
    """Test that null strategy returns input unchanged."""
    generator_output = {
        "prompt_token_ids": [[1, 2, 3], [4, 5, 6]],
        "response_ids": [[7, 8], [9, 10]],
        "rewards": [[1.0, 2.0], [3.0, 4.0]],
        "loss_masks": [[1, 1], [1, 1]],
        "stop_reasons": ["stop", "stop"],
        "rollout_metrics": None,
        "rollout_logprobs": [[0.16, 0.4], [0.2, 0.3]],
    }
    uids = ["uid1", "uid2"]
    sampling_config = {"type": None}

    result_output, result_uids, keep_sampling, state = handle_dynamic_sampling(generator_output, uids, sampling_config)

    assert result_output == generator_output
    assert result_uids == uids
    assert keep_sampling is False
    assert state is None


def test_handle_dynamic_sampling_invalid_strategy():
    """Test that invalid strategy raises ValueError."""
    generator_output = {
        "prompt_token_ids": [[1, 2, 3]],
        "response_ids": [[7, 8]],
        "rewards": [[1.0, 2.0]],
        "loss_masks": [[1, 1]],
        "rollout_logprobs": [[0.16, 0.4]],
    }
    uids = ["uid1"]
    sampling_config = {"type": "invalid_strategy"}

    with pytest.raises(ValueError, match="Invalid dynamic sampling type: invalid_strategy"):
        handle_dynamic_sampling(generator_output, uids, sampling_config)


def test_handle_replace_sampling_sufficient_good_samples():
    """Test replace sampling when there are sufficient good samples (>0.3)."""
    # Create test data with some good UIDs (high variance) and some bad UIDs (zero variance)
    generator_output = {
        "prompt_token_ids": [[1, 2], [1, 2], [3, 4], [3, 4], [5, 6], [5, 6]],
        "response_ids": [[13, 14], [15, 16], [17, 18], [19, 20], [21, 22], [23, 24]],
        "rewards": [
            1.0,
            2.0,
            1.0,
            1.0,
            3.0,
            4.0,
        ],  # uid1: [1.0, 2.0] (good), uid2: [1.0, 1.0] (bad), uid3: [3.0, 4.0] (good)
        "loss_masks": [[1, 1]] * 6,
        "stop_reasons": ["length"] * 6,
        "rollout_metrics": None,
        "rollout_logprobs": [[0.1, 0.2], [0.3, 0.4], [0.5, 0.25], [0.15, 0.25], [0.1, 0.2], [0.3, 0.4]],
    }
    uids = ["uid1", "uid1", "uid2", "uid2", "uid3", "uid3"]  # 2 samples per prompt
    sampling_config = {"n_samples_per_prompt": 2, "min_replace_ratio": 0.3}

    result_output, result_uids, keep_sampling = handle_replace_sampling(generator_output, uids, sampling_config)

    # Should not keep sampling
    assert keep_sampling is False

    # Output should have same structure but with replacements
    assert len(result_output["prompt_token_ids"]) == 6
    assert len(result_output["response_ids"]) == 6
    assert len(result_output["rewards"]) == 6
    assert len(result_output["rollout_logprobs"]) == 6
    assert len(result_uids) == 6

    # Check that bad uid2 samples were replaced with good samples
    uid2_indices = [i for i, uid in enumerate(result_uids) if uid == "uid2"]
    # After replacement, uid2 indices should now contain UIDs from good samples
    assert len(uid2_indices) == 0  # uid2 should be completely replaced


def test_handle_replace_sampling_insufficient_good_samples():
    """Test replace sampling when there are insufficient good samples (<0.3)."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [1, 2], [3, 4], [3, 4]],
        "response_ids": [[9, 10], [11, 12], [13, 14], [15, 16]],
        "rewards": [1.0, 1.0, 2.0, 2.0],  # uid1: [1.0, 1.0] (bad), uid2: [2.0, 2.0] (bad)
        "loss_masks": [[1, 1]] * 4,
        "stop_reasons": ["length"] * 4,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids = ["uid1", "uid1", "uid2", "uid2"]  # 2 samples per prompt
    sampling_config = {"n_samples_per_prompt": 2, "min_replace_ratio": 0.3}

    result_output, result_uids, keep_sampling = handle_replace_sampling(generator_output, uids, sampling_config)

    # Should keep sampling due to insufficient good samples
    assert keep_sampling is True

    # Output should be unchanged
    assert result_output == generator_output
    assert result_uids == uids


def test_handle_replace_sampling_single_sample_per_prompt():
    """Test replace sampling with single sample per prompt (should always be considered good)."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [3, 4]],
        "response_ids": [[5, 6], [7, 8]],
        "rewards": [1.0, 1.0],
        "loss_masks": [[1, 1]] * 2,
        "stop_reasons": ["stop", "stop"],
        "rollout_metrics": None,
        "rollout_logprobs": [[0.1, 0.2]],
    }
    uids = ["uid1", "uid2"]
    sampling_config = {"n_samples_per_prompt": 1, "min_replace_ratio": 0.3}

    result_output, result_uids, keep_sampling = handle_replace_sampling(generator_output, uids, sampling_config)

    # Should not keep sampling (single samples are always considered good)
    assert keep_sampling is False

    # Output should be unchanged since all samples are good
    assert result_output == generator_output
    assert result_uids == uids


def test_handle_replace_sampling_token_level_rewards():
    """Test replace sampling with token-level rewards (should sum to sequence level)."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [1, 2], [3, 4], [3, 4]],
        "response_ids": [[9, 10], [11, 12, 13], [14, 15], [16]],
        "rewards": [[1.0, 2.0], [3.0, 4.0, 5.0], [1.0, 1.0], [1.0]],  # Token-level rewards
        "loss_masks": [[1, 1]] * 4,
        "stop_reasons": ["stop"] * 4,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids = ["uid1", "uid1", "uid2", "uid2"]  # uid1: [3.0, 7.0] (good), uid2: [2.0, 2.0] (bad)
    sampling_config = {"n_samples_per_prompt": 2, "min_replace_ratio": 0.3}

    result_output, result_uids, keep_sampling = handle_replace_sampling(generator_output, uids, sampling_config)

    # Should not keep sampling (sufficient good samples)
    assert keep_sampling is False

    # Check that replacements occurred
    assert len(result_output["rewards"]) == 4
    assert len(result_uids) == 4


def test_handle_filter_sampling_sufficient_prompts():
    """Test filter sampling when we get sufficient prompts in one batch."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [1, 2], [3, 4], [3, 4]],
        "response_ids": [[9, 10], [11, 12], [13, 14], [15, 16]],
        "rewards": [1.0, 2.0, 3.0, 3.0],  # uid1: [1.0, 2.0] (good), uid2: [3.0, 3.0] (bad)
        "loss_masks": [[1, 1]] * 4,
        "stop_reasons": ["stop"] * 4,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids = ["uid1", "uid1", "uid2", "uid2"]
    sampling_config = {
        "train_batch_size": 1,  # Only need 1 prompt
        "n_samples_per_prompt": 2,
        "max_sample_batches": 20,
    }

    result_output, result_uids, keep_sampling, state = handle_filter_sampling(
        generator_output, uids, sampling_config, collected_state={"sample_batch_count": 1}
    )

    # Should not keep sampling (sufficient prompts)
    assert keep_sampling is False
    assert state is None

    # Should only keep the good uid1 samples
    assert len(result_output["prompt_token_ids"]) == 2
    assert len(result_uids) == 2
    assert all(uid == "uid1" for uid in result_uids)


def test_handle_filter_sampling_insufficient_prompts_continue():
    """Test filter sampling when we need to continue sampling."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [3, 4]],
        "response_ids": [[5, 6], [7, 8]],
        "rewards": [1.0, 2.0],  # Only 1 good prompt
        "loss_masks": [[1, 1]] * 2,
        "stop_reasons": ["stop"] * 2,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids = ["uid1", "uid1"]
    sampling_config = {
        "train_batch_size": 2,  # Need 2 prompts
        "n_samples_per_prompt": 2,
        "max_sample_batches": 20,
    }

    collected_state = {"sample_batch_count": 1}

    result_output, result_uids, keep_sampling, state = handle_filter_sampling(
        generator_output, uids, sampling_config, collected_state=collected_state
    )

    # Should keep sampling (insufficient prompts)
    assert keep_sampling is True
    assert result_output is generator_output
    assert result_uids is uids
    assert state is not None
    assert state["num_prompts_in_batch"] == 1
    assert state["sample_batch_count"] == 1


def test_handle_filter_sampling_accumulation():
    """Test filter sampling accumulation across multiple batches."""
    # First batch
    generator_output1 = {
        "prompt_token_ids": [[1, 2], [3, 4]],
        "response_ids": [[5, 6], [7, 8]],
        "rewards": [1.0, 2.0],  # Good prompt
        "loss_masks": [[1, 1]] * 2,
        "stop_reasons": ["stop"] * 2,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids1 = ["uid1", "uid1"]

    # Second batch
    generator_output2 = {
        "prompt_token_ids": [[9, 10], [11, 12]],
        "response_ids": [[13, 14], [15, 16]],
        "rewards": [3.0, 4.0],  # Another good prompt
        "loss_masks": [[1, 1]] * 2,
        "stop_reasons": ["stop"] * 2,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids2 = ["uid2", "uid2"]

    sampling_config = {
        "train_batch_size": 2,  # Need 2 prompts
        "n_samples_per_prompt": 2,
        "max_sample_batches": 20,
    }

    collected_state = {"sample_batch_count": 1}

    # Process first batch
    result1_output, result1_uids, keep_sampling1, state1 = handle_filter_sampling(
        generator_output1, uids1, sampling_config, collected_state=collected_state
    )

    assert keep_sampling1 is True  # Need more prompts
    assert state1["num_prompts_in_batch"] == 1

    # Process second batch
    result2_output, result2_uids, keep_sampling2, state2 = handle_filter_sampling(
        generator_output2, uids2, sampling_config, collected_state=state1
    )

    assert keep_sampling2 is False  # Now have enough prompts
    assert state2 is None
    assert len(result2_output["prompt_token_ids"]) == 4  # Both batches combined
    assert len(result2_uids) == 4


def test_handle_filter_sampling_single_sample_per_prompt():
    """Test filter sampling with single sample per prompt."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [3, 4]],
        "response_ids": [[5, 6], [7, 8]],
        "rewards": [1.0, 1.0],  # Same rewards but single sample per prompt
        "loss_masks": [[1, 1]] * 2,
        "stop_reasons": ["stop"] * 2,
        "rollout_metrics": None,
        "rollout_logprobs": None,
    }
    uids = ["uid1", "uid2"]  # Different UIDs, single sample each
    sampling_config = {
        "train_batch_size": 2,
        "n_samples_per_prompt": 1,
        "max_sample_batches": 20,
    }

    result_output, result_uids, keep_sampling, state = handle_filter_sampling(
        generator_output, uids, sampling_config, collected_state={"sample_batch_count": 1}
    )

    # Should not keep sampling (single samples are always kept)
    assert keep_sampling is False
    assert state is None
    assert len(result_output["prompt_token_ids"]) == 2
    assert len(result_uids) == 2


def test_filter_generator_output():
    """Test the filter_generator_output utility function."""
    generator_output = {
        "prompt_token_ids": [[1, 2], [3, 4], [5, 6]],
        "response_ids": [[7, 8], [9, 10], [11, 12]],
        "rewards": [1.0, 2.0, 3.0],
        "loss_masks": [[1, 1]] * 3,
        "stop_reasons": ["length", "length", "stop"],
        "rollout_metrics": {"metric": "value"},
        "rollout_logprobs": [[0.16, 0.4], [0.1, 0.2], [0.3, 0.4]],
    }
    kept_indices = [0, 2]  # Keep first and third samples

    filtered = filter_generator_output(generator_output, kept_indices)

    assert filtered["prompt_token_ids"] == [[1, 2], [5, 6]]
    assert filtered["response_ids"] == [[7, 8], [11, 12]]
    assert filtered["rewards"] == [1.0, 3.0]
    assert filtered["loss_masks"] == [[1, 1]] * 2
    assert filtered["stop_reasons"] == ["length", "stop"]
    assert filtered["rollout_metrics"] == {"metric": "value"}
    assert filtered["rollout_logprobs"] == [[0.16, 0.4], [0.3, 0.4]]


def test_zero_variance_filter_mixed_groups():
    """uid groups with non-zero variance and singletons are kept; zero-variance duplicates are removed."""
    rewards = [1.0, 2.0, 3.0, 3.0, 5.0]
    uids = ["uid1", "uid1", "uid2", "uid2", "uid3"]

    kept_indices = zero_variance_filter(rewards, uids)

    # uid1 has variance > 0 -> keep indices 0,1; uid2 has variance 0 with size>1 -> drop 2,3; uid3 singleton -> keep 4
    assert kept_indices == [0, 1, 4]


def test_zero_variance_filter_all_zero_variance_duplicates():
    """All duplicate groups with zero variance should be filtered out, yielding no kept indices."""
    rewards = [1.0, 1.0, 2.0, 2.0]
    uids = ["a", "a", "b", "b"]

    kept_indices = zero_variance_filter(rewards, uids)

    assert kept_indices == []


def test_zero_variance_filter_singletons_kept():
    """Singleton groups should always be kept regardless of reward values."""
    rewards = [1.0, 1.0, 1.0]
    uids = ["x", "y", "z"]

    kept_indices = zero_variance_filter(rewards, uids)

    assert kept_indices == [0, 1, 2]


def test_validate_generator_output_valid_case():
    """Test validate_generator_output with valid case."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2", "prompt3"],
        env_classes=["env1", "env2", "env3"],
        env_extras=[{"extra": 1}, {"extra": 2}, {"extra": 3}],
        sampling_params={"temperature": 0.7},
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3, 4], [5, 6], [7, 8, 9]],
        response_ids=[[10, 11, 12], [13, 14], [15]],
        rewards=[[0.5, 0.6, 0.7], [0.8, 0.9], [1.0]],  # Nested list structure
        loss_masks=[[1, 1, 0], [1, 1], [0]],
        stop_reasons=["stop", "length", "stop"],
        rollout_metrics={"metric1": 0.5, "metric2": 0.6},
        rollout_logprobs=None,
    )

    # Should not raise any exceptions
    validate_generator_output(len(input_batch["prompts"]), generator_output)

    # per trajectory rewards should work too
    generator_output["rewards"] = [0.5, 0.6, 0.7]
    validate_generator_output(len(input_batch["prompts"]), generator_output)

    # valid rollout logprobs
    generator_output["rollout_logprobs"] = [[0.11, 0.12, 0.13], [0.2, 0.3], [0.4]]
    validate_generator_output(len(input_batch["prompts"]), generator_output)


def test_validate_generator_output_empty_response_ids():
    """Test validate_generator_output raises RuntimeError when response_ids is empty."""
    input_batch = GeneratorInput(prompts=["prompt1"], env_classes=["env1"], env_extras=None, sampling_params=None)

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3]],
        response_ids=[],
        rewards=[],
        loss_masks=[],
        stop_reasons=[],
        rollout_logprobs=[],  # Empty response_ids
    )

    with pytest.raises(RuntimeError, match="No outputs generated"):
        validate_generator_output(len(input_batch["prompts"]), generator_output)


def test_validate_generator_output_mismatched_prompts_responses():
    """Test validate_generator_output raises AssertionError when prompts and response_ids lengths don't match."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2", "prompt3"],  # 3 prompts
        env_classes=["env1", "env2", "env3"],
        env_extras=None,
        sampling_params=None,
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2], [3, 4]],
        response_ids=[[7, 8], [9, 10]],  # Only 2 responses
        rewards=[0.5, 0.7],
        loss_masks=[[1, 1], [1, 0]],
        stop_reasons=["eos", "eos"],
        rollout_logprobs=None,
    )

    with pytest.raises(AssertionError, match=re.escape("Mismatch between prompts (3) and responses (2)")):
        validate_generator_output(len(input_batch["prompts"]), generator_output)


def test_validate_generator_output_all_loss_masked():
    """Test validate_generator_output logs warning when all outputs are loss masked."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2"], env_classes=["env1", "env2"], env_extras=None, sampling_params=None
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3], [4, 5, 6]],
        response_ids=[[7, 8], [9, 10]],
        rewards=[0.5, 0.7],
        loss_masks=[[0, 0], [0, 0]],  # All zeros - completely loss masked
        stop_reasons=["eos", "eos"],
        rollout_logprobs=None,
    )

    # Capture log output to verify warning is issued
    with patch("skyrl_train.utils.trainer_utils.logger") as mock_logger:
        validate_generator_output(len(input_batch["prompts"]), generator_output)
        mock_logger.warning.assert_called_once_with(
            "All outputs are loss masked, which may lead to NaN loss, please check your generation logic!!"
        )


def test_validate_generator_output_mismatched_list_lengths():
    """Test validate_generator_output raises AssertionError when generator output lists have mismatched lengths."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2"], env_classes=["env1", "env2"], env_extras=None, sampling_params=None
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3], [4, 5, 6]],
        response_ids=[[7, 8], [9, 10]],  # Length 2
        rewards=[0.5, 0.7, 0.9],  # Length 3 - mismatch!
        loss_masks=[[1, 1], [1, 0]],
        stop_reasons=["eos", "eos"],
        rollout_logprobs=None,
    )

    with pytest.raises(AssertionError, match="Generator output rewards length must be equal to response_ids length"):
        validate_generator_output(len(input_batch["prompts"]), generator_output)


def test_validate_generator_output_element_length_mismatch():
    """Test validate_generator_output with element length mismatch."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2", "prompt3"],
        env_classes=["env1", "env2", "env3"],
        env_extras=[{"extra": 1}, {"extra": 2}, {"extra": 3}],
        sampling_params={"temperature": 0.7},
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3, 4], [5, 6], [7, 8, 9]],
        response_ids=[[10, 11, 12], [13, 14], [15]],
        rewards=[[0.5, 0.6, 0.7], [0.8, 0.9], [1.0]],
        loss_masks=[[1, 1], [1], [1, 1]],  # loss masks are not the same length as response ids
        stop_reasons=["stop", "length", "stop"],
        rollout_metrics={"metric1": 0.5, "metric2": 0.6},
        rollout_logprobs=None,
    )

    with pytest.raises(AssertionError, match="Response ids and loss masks must have the same length"):
        validate_generator_output(len(input_batch["prompts"]), generator_output)

    generator_output["loss_masks"] = [[1, 1, 1], [1, 1], [1]]  # add correct loss masks
    generator_output["rewards"] = [[0.5, 0.6], [0.8], [1.0, 2.0]]  # add incorrect rewards
    with pytest.raises(AssertionError, match="Token rewards and response ids must have the same length"):
        validate_generator_output(len(input_batch["prompts"]), generator_output)

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3], [4, 5, 6], [7, 8, 9]],
        response_ids=[[7, 8], [9, 10], [11, 12]],
        rewards=[0.5, 0.7, -0.1],
        loss_masks=[[1, 1], [1, 0], [1, 1]],
        stop_reasons=["eos", "eos", "length"],
        rollout_logprobs=[[0.17, 0.2], [0.9], [0.1, 0.2]],  # Second entry has length 1 - mismatch !
    )

    with pytest.raises(AssertionError, match="Response ids and rollout logprobs must have the same length"):
        validate_generator_output(len(input_batch["prompts"]), generator_output)


def test_build_dataloader_seeding(dummy_config):
    """Test that build_dataloader correctly seeds the dataloader for reproducible shuffling."""

    # Create a dataset with multiple distinct items to test shuffling
    class MultiItemDataset:
        def __init__(self, size=10):
            self.data = [f"item_{i}" for i in range(size)]

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return self.data[idx]

        def collate_fn(self, batch):
            return batch

    dataset = MultiItemDataset(size=20)

    # Test 1: Same seed should produce same shuffling
    config1 = dummy_config.copy()
    config1.trainer.seed = 42
    config1.trainer.train_batch_size = 5

    config2 = dummy_config.copy()
    config2.trainer.seed = 42  # Same seed
    config2.trainer.train_batch_size = 5

    # Build dataloaders
    dataloader1 = build_dataloader(config1, dataset, is_train=True)
    dataloader2 = build_dataloader(config2, dataset, is_train=True)

    # Get first batch from each dataloader
    first_batch1 = next(iter(dataloader1))
    first_batch2 = next(iter(dataloader2))

    # With same seed, first batches should be identical
    assert (
        first_batch1 == first_batch2
    ), f"Same seed should produce same first batch, got {first_batch1} vs {first_batch2}"

    # Test 2: Different seeds should produce different shuffling
    config3 = dummy_config.copy()
    config3.trainer.seed = 123  # Different seed
    config3.trainer.train_batch_size = 5

    dataloader3 = build_dataloader(config3, dataset, is_train=True)
    first_batch3 = next(iter(dataloader3))

    # With different seed, first batch should be different
    # Note: There's a tiny chance they could be the same by random chance, but very unlikely with 20 items
    assert (
        first_batch1 != first_batch3
    ), f"Different seeds should produce different first batches, but both gave {first_batch1}"


def test_validate_generator_output_invalid_rewards():
    """Test validate_generator_output raises AssertionError when rewards is neither List[float-like] nor List[List[float-like]]."""
    input_batch = GeneratorInput(
        prompts=["prompt1", "prompt2"], env_classes=["env1", "env2"], env_extras=None, sampling_params=None
    )

    generator_output = GeneratorOutput(
        prompt_token_ids=[[1, 2, 3], [4, 5, 6]],
        response_ids=[[7, 8], [9, 10]],
        rewards=[[0.5, 0.6], 0.7],
        loss_masks=[[1, 1], [1, 0]],
        stop_reasons=["eos", "eos"],
        rollout_logprobs=None,
    )

    with pytest.raises(
        AssertionError,
        match=re.escape("rewards must be `List[float]` or `List[List[float]]`"),
    ):
        validate_generator_output(len(input_batch["prompts"]), generator_output)

    generator_output["rewards"] = [0.5, 0.7]
    validate_generator_output(len(input_batch["prompts"]), generator_output)

    generator_output["rewards"] = [[0.5, 0.6], [0.7, 0.8]]
    validate_generator_output(len(input_batch["prompts"]), generator_output)


# ============================================================================
# HybridEnvSampler Tests
# ============================================================================


class MockPromptDataset:
    """Mock PromptDataset for testing HybridEnvSampler."""

    def __init__(self, env_distribution: dict):
        """
        Create a mock dataset with specified environment distribution.

        Args:
            env_distribution: Dict mapping env_class -> number of samples
                e.g., {"booking": 10, "github": 5, "reddit": 3}
        """
        self.env_class_key = "env_class"
        self.dataframe = []

        for env_class, count in env_distribution.items():
            for i in range(count):
                self.dataframe.append(
                    {
                        "env_class": env_class,
                        "prompt": f"prompt_{env_class}_{i}",
                    }
                )

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe[idx]
        return row["prompt"], row["env_class"], {}, str(idx)

    def collate_fn(self, batch):
        return batch


def test_hybrid_env_sampler_minimum_samples_per_env():
    """Test that HybridEnvSampler ensures minimum samples from each environment."""
    # Create dataset with uneven distribution
    dataset = MockPromptDataset(
        {
            "booking": 100,
            "github": 50,
            "reddit": 20,
        }
    )

    batch_size = 30
    min_samples_per_env = 3

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
        drop_last=True,
    )

    # Get first batch (sampler yields batches of indices)
    first_batch = next(iter(sampler))
    batch_indices = first_batch

    # Count samples per env in the batch
    env_counts = {"booking": 0, "github": 0, "reddit": 0}
    for idx in batch_indices:
        env_class = dataset.dataframe[idx]["env_class"]
        env_counts[env_class] += 1

    # Verify minimum samples from each environment
    for env, count in env_counts.items():
        assert (
            count >= min_samples_per_env
        ), f"Environment {env} has {count} samples, expected at least {min_samples_per_env}"

    # Verify total batch size
    assert len(batch_indices) == batch_size


def test_hybrid_env_sampler_reproducibility():
    """Test that HybridEnvSampler produces same results with same seed."""
    dataset = MockPromptDataset(
        {
            "env_a": 20,
            "env_b": 15,
            "env_c": 10,
        }
    )

    batch_size = 12
    min_samples_per_env = 2

    # Create two samplers with same seed
    sampler1 = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(123),
    )

    sampler2 = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(123),
    )

    # Get first batch from both (sampler yields batches)
    batch1 = next(iter(sampler1))
    batch2 = next(iter(sampler2))

    # Should be identical
    assert batch1 == batch2, "Same seed should produce identical batches"


def test_hybrid_env_sampler_different_seeds():
    """Test that different seeds produce different results."""
    dataset = MockPromptDataset(
        {
            "env_a": 30,
            "env_b": 25,
            "env_c": 20,
        }
    )

    batch_size = 15
    min_samples_per_env = 2

    sampler1 = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    sampler2 = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(999),
    )

    # Get first batch from each (sampler yields batches)
    batch1 = next(iter(sampler1))
    batch2 = next(iter(sampler2))

    # Should be different (with very high probability)
    assert batch1 != batch2, "Different seeds should produce different batches"


def test_hybrid_env_sampler_warns_when_min_exceeds_batch():
    """Test that sampler warns and adjusts when min_samples_per_env * num_envs > batch_size."""
    dataset = MockPromptDataset(
        {
            "env_a": 10,
            "env_b": 10,
            "env_c": 10,
            "env_d": 10,
            "env_e": 10,
        }
    )

    # 5 envs * 5 min = 25 > batch_size of 10
    batch_size = 10
    min_samples_per_env = 5

    # Should warn and reduce min_samples_per_env
    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # min_samples_per_env should be reduced to fit: max(1, 10 // 5) = 2
    assert sampler.min_samples_per_env == 2


def test_hybrid_env_sampler_length():
    """Test that __len__ returns correct number of batches."""
    dataset = MockPromptDataset(
        {
            "env_a": 20,
            "env_b": 10,  # Smallest: limits batches
            "env_c": 15,
        }
    )

    batch_size = 9
    min_samples_per_env = 2

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # env_b has 10 samples, needs 2 per batch -> 5 batches possible
    # __len__ returns number of batches (for batch_sampler compatibility)
    expected_batches = 10 // min_samples_per_env  # 5

    assert len(sampler) == expected_batches


def test_hybrid_env_sampler_proportional_fill():
    """Test that remaining slots are filled proportionally."""
    # Create dataset with very uneven distribution
    dataset = MockPromptDataset(
        {
            "large_env": 900,  # 90%
            "small_env": 100,  # 10%
        }
    )

    batch_size = 100
    min_samples_per_env = 1

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # Collect multiple batches to get statistical average
    # Sampler yields batches, so flatten them
    all_batches = list(sampler)
    num_batches = len(all_batches)

    total_large = 0
    total_small = 0

    for batch in all_batches:
        for idx in batch:
            env = dataset.dataframe[idx]["env_class"]
            if env == "large_env":
                total_large += 1
            else:
                total_small += 1

    # After minimum allocation (1 each), 98 slots filled proportionally
    # Expected: ~90% large, ~10% small (with some variance)
    total = total_large + total_small
    large_ratio = total_large / total

    # Should be roughly 90% large (allowing Â±10% tolerance for randomness)
    assert 0.80 <= large_ratio <= 0.95, f"Expected ~90% large_env samples, got {large_ratio*100:.1f}%"


def test_hybrid_env_sampler_single_env():
    """Test sampler with only one environment."""
    dataset = MockPromptDataset(
        {
            "only_env": 50,
        }
    )

    batch_size = 10
    min_samples_per_env = 1

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # Get first batch (sampler yields batches)
    batch = next(iter(sampler))

    # All should be from the only environment
    assert len(batch) == batch_size
    for idx in batch:
        assert dataset.dataframe[idx]["env_class"] == "only_env"


def test_hybrid_env_sampler_many_small_envs():
    """Test sampler with many small environments."""
    # 10 environments with 5 samples each
    env_dist = {f"env_{i}": 5 for i in range(10)}
    dataset = MockPromptDataset(env_dist)

    batch_size = 20
    min_samples_per_env = 1

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # Get first batch (sampler yields batches)
    batch = next(iter(sampler))

    # Count envs in batch
    env_counts = {}
    for idx in batch:
        env = dataset.dataframe[idx]["env_class"]
        env_counts[env] = env_counts.get(env, 0) + 1

    # Each env should have at least 1 sample
    for env in env_dist.keys():
        assert env in env_counts, f"Environment {env} missing from batch"
        assert env_counts[env] >= min_samples_per_env


def test_hybrid_env_sampler_iteration_exhaustion():
    """Test that sampler correctly limits iterations based on smallest env."""
    dataset = MockPromptDataset(
        {
            "abundant": 100,
            "scarce": 6,  # Only 6 samples
        }
    )

    batch_size = 10
    min_samples_per_env = 2

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # scarce has 6, needs 2 per batch -> 3 batches max
    all_batches = list(sampler)

    expected_num_batches = 3
    assert len(all_batches) == expected_num_batches, f"Expected {expected_num_batches} batches, got {len(all_batches)}"

    # Verify total samples
    total_samples = sum(len(batch) for batch in all_batches)
    expected_total = 3 * batch_size  # 30
    assert total_samples == expected_total, f"Expected {expected_total} samples, got {total_samples}"


def test_hybrid_env_sampler_dataloader_integration():
    """
    Test that HybridEnvSampler works correctly with DataLoader using batch_sampler.

    This test catches the bug where using `sampler=` instead of `batch_sampler=`
    caused the collate_fn to receive indices instead of dataset items, resulting in:
        ValueError: not enough values to unpack (expected 4, got 1)
    """
    from torch.utils.data import DataLoader

    dataset = MockPromptDataset(
        {
            "booking": 20,
            "github": 15,
            "reddit": 10,
        }
    )

    batch_size = 9
    min_samples_per_env = 2

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=batch_size,
        min_samples_per_env=min_samples_per_env,
        generator=torch.Generator().manual_seed(42),
    )

    # This is the correct way - using batch_sampler
    dataloader = DataLoader(
        dataset,
        batch_sampler=sampler,
        collate_fn=dataset.collate_fn,
    )

    # Get first batch
    batch = next(iter(dataloader))

    # Verify batch structure - should be list of tuples from __getitem__
    assert len(batch) == batch_size, f"Expected batch size {batch_size}, got {len(batch)}"

    # Each item should be a tuple of (prompt, env_class, extras, uid)
    for item in batch:
        assert isinstance(item, tuple), f"Expected tuple, got {type(item)}"
        assert len(item) == 4, f"Expected 4 elements (prompt, env_class, extras, uid), got {len(item)}"
        prompt, env_class, extras, uid = item
        assert isinstance(prompt, str), f"Expected prompt to be str, got {type(prompt)}"
        assert env_class in ["booking", "github", "reddit"], f"Unexpected env_class: {env_class}"
        assert isinstance(extras, dict), f"Expected extras to be dict, got {type(extras)}"
        assert isinstance(uid, str), f"Expected uid to be str, got {type(uid)}"

    # Verify minimum samples per environment in the batch
    env_counts = {}
    for item in batch:
        env_class = item[1]
        env_counts[env_class] = env_counts.get(env_class, 0) + 1

    for env, count in env_counts.items():
        assert count >= min_samples_per_env, f"Environment {env} has {count} samples, expected >= {min_samples_per_env}"


def test_hybrid_env_sampler_wrong_usage_fails():
    """
    Demonstrate that using sampler= instead of batch_sampler= fails.

    This documents the bug we fixed - the sampler yields batches of indices,
    not individual indices, so it must be used with batch_sampler=.
    """
    from torch.utils.data import DataLoader

    dataset = MockPromptDataset({"env_a": 10, "env_b": 10})

    sampler = HybridEnvSampler(
        dataset=dataset,
        batch_size=6,
        min_samples_per_env=1,
        generator=torch.Generator().manual_seed(42),
    )

    # WRONG: Using sampler= with a batch-yielding sampler and batch_size
    # This causes the dataloader to:
    # 1. Get a batch [idx1, idx2, ...] from sampler
    # 2. Try to use each element as an index to fetch from dataset
    # 3. But since sampler yields whole batches, not indices, this fails
    wrong_dataloader = DataLoader(
        dataset,
        batch_size=6,  # DataLoader will try to batch the batches
        sampler=sampler,
        collate_fn=dataset.collate_fn,
    )

    # This will fail because:
    # - sampler yields [0, 3, 5, 1, 4, 2] (a batch of indices)
    # - DataLoader tries to use [0, 3, 5, 1, 4, 2] as the index for __getitem__
    # - dataset[list] fails with TypeError
    with pytest.raises(TypeError):
        next(iter(wrong_dataloader))
