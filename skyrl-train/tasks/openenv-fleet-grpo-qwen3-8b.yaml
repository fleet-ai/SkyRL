# Fleet Task GRPO Training via SkyPilot - Qwen3-8B
# Usage: sky launch tasks/openenv-fleet-grpo-qwen3-8b.yaml --env FLEET_API_KEY=<key> --env WANDB_API_KEY=<key>
#
# This task trains GRPO on Fleet-hosted environments using OpenEnv's FleetTaskEnv.
# It loads tasks from the Fleet API and runs training with verifier-based rewards.
# Checkpoints are automatically uploaded to S3 to prevent disk exhaustion.
#
# Model: Qwen/Qwen3-8B (8B parameters, instruct-tuned)
# GPUs: B200:2 (preferred) or H100:4 (fallback)

name: fleet-task-grpo-qwen3-8b

resources:
  disk_size: 200  # Needs space for: model (~16GB), optimizer (~32GB), Ray temp (~50GB), vLLM cache, checkpoints
  ports: 6479
  any_of:
    # Prefer B200 (2x needed for 8B model)
    - accelerators: B200:2
      cloud: lambda
    - accelerators: B200:2
      cloud: runpod
    - accelerators: B200:2
      cloud: vast
    - accelerators: B200:2
      cloud: primeintellect
    # Fallback to H100 (4x needed for 8B model)
    - accelerators: H100:4
      cloud: lambda
    - accelerators: H100:4
      cloud: runpod
    - accelerators: H100:4
      cloud: vast
    - accelerators: H100:4
      cloud: primeintellect

num_nodes: 1

workdir:
  url: https://github.com/fleet-ai/SkyRL.git
  ref: main

envs:
  # These are passed via --env from GHA workflow
  WANDB_API_KEY: ""
  FLEET_API_KEY: ""
  LOGGER: "wandb"
  INFERENCE_BACKEND: "vllm"
  # Task selection: env_key to filter (e.g., "github", "booking", "reddit")
  ENV_KEY: ""
  # Modality: "tool_use" or "computer_use"
  MODALITY: "tool_use"
  # Training parameters
  MAX_TURNS: 50
  NUM_EPOCHS: 20
  RUN_ID: ""
  # Optional: limit number of tasks for testing
  MAX_TASKS: ""
  # AWS credentials (required for S3 dataset download, optional for checkpoint upload)
  AWS_ACCESS_KEY_ID: ""
  AWS_SECRET_ACCESS_KEY: ""
  AWS_REGION: "us-east-1"
  # S3 buckets
  S3_DATASET_BUCKET: "fleet-internal-datasets"
  S3_CHECKPOINT_BUCKET: "skyrl-checkpoints"

setup: |
  set -euo pipefail
  cd skyrl-train

  # Validate required environment variables
  echo "Validating environment variables..."
  if [ -z "$FLEET_API_KEY" ]; then
    echo "ERROR: FLEET_API_KEY is required"
    exit 1
  fi
  if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ]; then
    echo "ERROR: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required for S3 dataset download"
    exit 1
  fi
  if [ "$MODALITY" != "tool_use" ] && [ "$MODALITY" != "computer_use" ]; then
    echo "ERROR: MODALITY must be 'tool_use' or 'computer_use', got: $MODALITY"
    exit 1
  fi
  echo "Environment validation passed"

  uv venv --python 3.12 --seed
  source .venv/bin/activate
  uv sync --extra vllm
  uv pip install wandb boto3 awscli

  # Install OpenEnv for Fleet environment access (from branch with FleetTaskEnv)
  uv pip install "git+https://github.com/fleet-ai/OpenEnv.git@deniz/fleet_client" fleet-python

  # Download dataset from S3
  mkdir -p $HOME/data/fleet
  TASKS_FILE="$HOME/data/fleet/tasks_${MODALITY}.json"
  S3_PATH="s3://${S3_DATASET_BUCKET}/v0.1/openenv/all_${MODALITY}.json"
  echo "Downloading dataset from $S3_PATH..."
  aws s3 cp "$S3_PATH" "$TASKS_FILE"
  TASK_COUNT=$(python3 -c "import json; print(len(json.load(open('$TASKS_FILE'))['tasks']))")
  echo "Downloaded $TASK_COUNT tasks for modality: $MODALITY"

  # Prepare dataset
  DATA_DIR="$HOME/data/fleet/${MODALITY}"
  uv run python -m integrations.fleet.prepare_dataset \
    --tasks-json "$TASKS_FILE" \
    --output-dir "$DATA_DIR" \
    --modality "$MODALITY"

run: |
  set -euo pipefail
  cd skyrl-train
  source .venv/bin/activate

  TMP_DIR="$HOME/skyrl-tmp"
  mkdir -p "$TMP_DIR"
  export TMPDIR="$TMP_DIR"

  TASKS_FILE="$HOME/data/fleet/tasks_${MODALITY}.json"
  DATA_DIR="$HOME/data/fleet/${MODALITY}"

  # Login to Weights & Biases
  uv run -- python3 -c "import wandb; wandb.login(relogin=True, key='$WANDB_API_KEY')"

  # Ray cluster setup - limit object store to prevent disk exhaustion
  export RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook
  export RAY_object_store_memory=10000000000  # 10GB limit for object store
  if ! ray status --address 127.0.0.1:6479 >/dev/null 2>&1; then
    ray start --head --disable-usage-stats --port 6479 --object-store-memory=10000000000
  fi

  # Wait for Ray
  for i in $(seq 1 24); do
    if ray status --address 127.0.0.1:6479 >/dev/null 2>&1; then
      break
    fi
    sleep 5
  done

  TOTAL_GPUS=$SKYPILOT_NUM_GPUS_PER_NODE

  # Run GRPO training with Fleet task integration
  uv run --isolated --extra "$INFERENCE_BACKEND" \
    --with "litellm>=1.75.5" \
    --with "git+https://github.com/fleet-ai/OpenEnv.git@deniz/fleet_client" \
    --with "fleet-python" \
    --with "boto3" \
    -m integrations.fleet.entrypoints.main_fleet \
    data.train_data="['${DATA_DIR}/train.parquet']" \
    data.val_data="['${DATA_DIR}/validation.parquet']" \
    environment.env_class=fleet_task \
    environment.skyrl_gym.fleet_task.tasks_file="$TASKS_FILE" \
    trainer.algorithm.advantage_estimator="grpo" \
    trainer.policy.model.path="Qwen/Qwen3-8B" \
    trainer.placement.colocate_all=true \
    trainer.strategy=fsdp2 \
    trainer.placement.policy_num_gpus_per_node=$TOTAL_GPUS \
    trainer.placement.ref_num_gpus_per_node=$TOTAL_GPUS \
    generator.num_inference_engines=$TOTAL_GPUS \
    generator.inference_engine_tensor_parallel_size=1 \
    trainer.epochs=${NUM_EPOCHS} \
    trainer.eval_batch_size=4 \
    trainer.eval_before_train=true \
    trainer.eval_interval=5 \
    trainer.update_epochs_per_batch=1 \
    trainer.train_batch_size=8 \
    trainer.policy_mini_batch_size=4 \
    trainer.micro_forward_batch_size_per_gpu=4 \
    trainer.micro_train_batch_size_per_gpu=4 \
    trainer.ckpt_interval=10 \
    trainer.max_prompt_length=512 \
    generator.sampling_params.max_generate_length=2048 \
    generator.sampling_params.temperature=0.6 \
    generator.sampling_params.top_p=0.95 \
    generator.sampling_params.stop='["</tool_call>", "<done>"]' \
    generator.eval_sampling_params.stop='["</tool_call>", "<done>"]' \
    trainer.policy.optimizer_config.lr=1.0e-6 \
    trainer.algorithm.use_kl_loss=true \
    generator.max_turns=$MAX_TURNS \
    generator.backend=$INFERENCE_BACKEND \
    generator.run_engines_locally=true \
    generator.weight_sync_backend=nccl \
    generator.async_engine=true \
    generator.batched=false \
    generator.use_conversation_multi_turn=true \
    generator.n_samples_per_prompt=4 \
    generator.eval_n_samples_per_prompt=3 \
    generator.gpu_memory_utilization=0.8 \
    trainer.logger="$LOGGER" \
    trainer.project_name="fleet-task-grpo" \
    trainer.run_name="fleet_${MODALITY}_${RUN_ID:-$(head -c 4 /dev/urandom | xxd -p)}" \
    trainer.resume_mode=latest \
    trainer.ckpt_path="$HOME/ckpts/fleet_${MODALITY}" \
    trainer.dump_data_batch=true
