# Fleet Task GRPO Training via SkyPilot - GLM-4.7-Flash
# Usage: sky launch tasks/openenv-fleet-grpo-glm-4.7-flash.yaml --env FLEET_API_KEY=<key> --env WANDB_API_KEY=<key>
#
# Zhipu AI GLM-4.7-Flash: Efficient MoE model for coding and agents
# - 30B total params, 3B active (MoE with 5/64 experts)
# - Multi-Headed Latent Attention (MLA)
# - GLM-4.7 family has strong tool-use capabilities
# - vLLM day-zero support
#
# Model: zai-org/GLM-4.7-Flash (30B-A3B MoE)
# GPUs: B200:8 (more parallelism for faster training)
#
# v0.1.0: Initial GLM-4.7-Flash config
# v0.1.1: Install transformers from source for glm4_moe_lite architecture support
# v0.1.2: Use vLLM nightly (required for GLM-4.7-Flash MoE support)
# v0.1.3: Add vLLM + transformers to uv run --with (--isolated doesn't see setup packages)
# v0.1.4: Use vLLM nightly - stable vLLM requires transformers<5 but
#         glm4_moe_lite architecture needs transformers 5.x from source
# v0.1.5: Install ALL deps in setup, don't use --isolated (fixes flash-attn ABI mismatch)
#         --isolated builds flash-attn from source with wrong PyTorch version
# v0.1.6: Install torchvision after vLLM (transformers 5.x needs compatible torchvision)
# v0.1.7: Force reinstall torchvision from PyTorch CUDA 12.8 index (B200 Blackwell)
# v0.1.8: Install torch+torchvision together from nightly/cu128 to ensure they match
# v0.1.9: Uninstall torchvision - text model doesn't need it (transformers#38065)
# v0.1.10: Install vLLM LAST so its PyTorch/C++ ABI is preserved
# v0.1.11: Uninstall torchvision AFTER vLLM (vLLM reinstalls it as dep)
# v0.1.12: Install vLLM FIRST, then transformers from source AFTER
#          vLLM has pinned deps that downgrade transformers to 4.57.x
#          Installing transformers after vLLM overrides this downgrade
# v0.1.13: Install vLLM from source (git+) instead of nightly wheels
#          Nightly wheels have availability issues, source build is more reliable
# v0.1.14: Revert to nightly wheels - source build requires CUDA toolkit (nvcc)
#          which RunPod images don't include (only CUDA runtime)
# v0.1.15: Use exact install command from HuggingFace model card
#          --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly
# v0.1.16: Enable preserved thinking for multi-turn tool use
#          reasoning_parser=glm45, tool_call_parser=glm47 (from HF model card)
# v0.1.17: Install vLLM BEFORE uv sync (from B200 debugging)
#          vLLM brings PyTorch 2.10.0, uv sync after won't downgrade it
# v0.1.18: Use uv sync --inexact to prevent removing vLLM
# v0.1.19: Replace uv sync with uv pip install -e . (uv sync downgrades PyTorch)
# v0.1.20: Increase GPU count from 4 to 8 for faster training
# v0.1.21: Fix GPU names per provider (H200-SXM for RunPod/Vast)
# v0.1.22: Remove Nebius (fabric config issues causing stuck launches)
# v0.1.23: Use --no-deps for editable install (uv pip install -e . still downgrades PyTorch)
# v0.1.24: Don't use 'uv run' in run phase (re-syncs lockfile, downgrades PyTorch)
# v0.1.25: Also remove 'uv run' from setup PREPARE_CMD (same issue)
# v0.1.26: Install missing deps (datasets, ray, etc.) since --no-deps skips them
# v0.1.27: Remove 'ray stop' (kills SkyPilot's Ray cluster that manages the job)
# v0.1.28: Add ALL deps from pyproject.toml (--no-deps skips them all)
# v0.1.29: Install skyrl-gym from local path (not PyPI - different package)
# v0.1.30: Use +generator.reasoning_parser (add new key to Hydra config)
# v0.1.31: DEBUG flash-attn build - check nvcc, CUDA, pyproject.toml settings
# v0.1.32: Skip flash-attn entirely, use FLASHINFER attention backend
#          - flash-attn has NO prebuilt wheels for PyTorch 2.10.0 (vLLM nightly)
#          - Building from source requires nvcc which RunPod doesn't have
#          - FLASHINFER is bundled with vLLM and works without external deps
#          - Set VLLM_ATTENTION_BACKEND=FLASHINFER in environment
# v0.1.33: Install flash-attn Python utilities (SKIP_CUDA_BUILD=TRUE)
#          - SkyRL training code needs flash_attn.bert_padding (pad_input/unpad_input)
#          - These are pure Python functions, no CUDA kernels needed
#          - vLLM still uses FLASHINFER for inference attention
# v0.1.34: Create mock flash_attn_2_cuda module
#          - flash_attn's __init__.py imports flash_attn_2_cuda at runtime
#          - Without CUDA build, this module doesn't exist causing ImportError
#          - Create mock module so flash_attn can import (we only need bert_padding)

name: fleet-task-grpo-glm-4.7-flash

resources:
  disk_size: 300
  ports: 6479
  any_of:
    # B200 options
    - accelerators: B200:8
      cloud: lambda
      memory: 750+
    - accelerators: B200:8
      cloud: runpod
      memory: 750+
    # H200 options (use H200-SXM for RunPod/Vast)
    - accelerators: H200-SXM:8
      cloud: runpod
      memory: 750+
    - accelerators: H200-SXM:8
      cloud: vast
      memory: 750+

num_nodes: 1

workdir:
  url: https://github.com/fleet-ai/SkyRL.git
  ref: main

envs:
  WANDB_API_KEY: ""
  FLEET_API_KEY: ""
  LOGGER: "wandb"
  INFERENCE_BACKEND: "vllm"
  # Use FLASHINFER attention backend for vLLM inference (bundled with vLLM)
  # flash-attn installed with SKIP_CUDA_BUILD for Python utilities only (bert_padding)
  VLLM_ATTENTION_BACKEND: "FLASHINFER"
  ENV_KEYS: ""
  MODALITY: "tool_use"
  MAX_TURNS: 50
  # GLM-4.7-Flash supports 128K context natively
  MAX_INPUT_LENGTH: 48000
  MAX_GENERATE_LENGTH: 8192
  NUM_EPOCHS: 20
  RUN_ID: ""
  MAX_TASKS: ""
  AWS_ACCESS_KEY_ID: ""
  AWS_SECRET_ACCESS_KEY: ""
  AWS_REGION: "us-east-1"
  S3_DATASET_BUCKET: "fleet-internal-datasets"
  S3_CHECKPOINT_BUCKET: "skyrl-checkpoints"
  S3_TRAJECTORY_BUCKET: "skyrl-trajectories"
  DATA_VERSION: "v0.3"

setup: |
  set -euo pipefail
  cd skyrl-train

  echo "Validating environment variables..."
  if [ -z "$FLEET_API_KEY" ]; then
    echo "ERROR: FLEET_API_KEY is required"
    exit 1
  fi
  if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ]; then
    echo "ERROR: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are required for S3 dataset download"
    exit 1
  fi
  if [ "$MODALITY" != "tool_use" ] && [ "$MODALITY" != "computer_use" ]; then
    echo "ERROR: MODALITY must be 'tool_use' or 'computer_use', got: $MODALITY"
    exit 1
  fi
  echo "Environment validation passed"

  uv venv --python 3.12 --seed
  source .venv/bin/activate

  # GLM-4.7-Flash requires specific install order:
  # 1. vLLM nightly FIRST - brings PyTorch 2.10.0 (vLLM compiled against this version)
  # 2. flash-attn with SKIP_CUDA_BUILD - for Python utilities (bert_padding) used by SkyRL
  # 3. transformers from source - for glm4_moe_lite architecture (overrides vLLM's 4.x pin)
  # 4. skyrl-gym from local - PyPI has different package with same name
  # 5. Other deps via pip (NOT uv sync - it enforces lockfile and downgrades PyTorch)
  #
  # Why not uv sync? The lockfile pins torch==2.9.0 but vLLM nightly needs torch==2.10.0
  # Why --no-deps for main package? Prevents installing vllm/transformers from PyPI
  # Why SKIP_CUDA_BUILD for flash-attn? No prebuilt CUDA wheels for torch 2.10, RunPod lacks nvcc
  #                                     We only need Python utilities, vLLM uses FLASHINFER

  # Step 1: Install vLLM nightly (HuggingFace model card command)
  # https://huggingface.co/zai-org/GLM-4.7-Flash
  uv pip install vllm --pre --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly

  # Step 2: Install flash-attn Python utilities only (no CUDA kernels)
  # SkyRL training code uses flash_attn.bert_padding.pad_input/unpad_input
  # We skip CUDA build since vLLM uses FLASHINFER backend for attention
  FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE uv pip install flash-attn --no-build-isolation

  # Step 2b: Create mock flash_attn_2_cuda module
  # flash_attn's __init__.py imports flash_attn_2_cuda which doesn't exist without CUDA build
  # We create a mock so the package can import (we only need bert_padding utilities)
  python3 -c "import site; p=site.getsitepackages()[0]+'/flash_attn_2_cuda.py'; open(p,'w').write('def _ni(*a,**k): raise NotImplementedError(\"flash_attn CUDA not available\")\\nfwd=bwd=varlen_fwd=varlen_bwd=flash_attn_func=flash_attn_varlen_func=flash_attn_with_kvcache=_ni\\n'); print(f'Created mock at {p}')"

  # Step 3: Install transformers from source for glm4_moe_lite architecture
  uv pip install git+https://github.com/huggingface/transformers.git

  # Step 4: Install skyrl-gym from LOCAL path (NOT PyPI - different package!)
  uv pip install -e ../skyrl-gym

  # Step 5: Install main package without deps (deps installed separately to avoid version conflicts)
  uv pip install -e . --no-deps

  # Step 6: Install remaining core deps from pyproject.toml (--no-deps skipped these)
  uv pip install loguru tqdm ninja tensorboard func_timeout hydra-core==1.3.2 accelerate torchdata omegaconf ray==2.51.1 peft debugpy==1.8.0 hf_transfer datasets==4.0.0 tensordict jaxtyping polars s3fs fastapi uvicorn pybind11 setuptools pyarrow

  # Step 7: Install runtime deps
  uv pip install "litellm>=1.75.5" boto3 wandb awscli
  uv pip install "git+https://github.com/fleet-ai/OpenEnv.git@deniz/fleet_client" fleet-python

  # Cleanup: Uninstall torchvision (vLLM installs it but text models don't need it)
  uv pip uninstall torchvision || true

  # Diagnostics: print versions and test imports
  echo "=== DIAGNOSTIC: Package versions ==="
  pip show torch | grep -E "^(Name|Version)"
  pip show transformers | grep -E "^(Name|Version)"
  pip show vllm | grep -E "^(Name|Version)"
  pip show flash-attn | grep -E "^(Name|Version)"
  pip show skyrl-gym | grep -E "^(Name|Version|Location)"
  echo "=== DIAGNOSTIC: Test imports ==="
  python3 -c "import torch; print(f'torch: {torch.__version__}')"
  python3 -c "import transformers; print(f'transformers: {transformers.__version__}')"
  python3 -c "import vllm; print(f'vllm: {vllm.__version__}')"
  python3 -c "from flash_attn.bert_padding import pad_input, unpad_input; print('flash_attn.bert_padding: OK')"
  python3 -c "import skyrl_gym; print(f'skyrl_gym loaded from: {skyrl_gym.__file__}')"
  echo "=== DIAGNOSTIC: Test vLLM CUDA module ==="
  python3 -c "import vllm._C; print('vllm._C imported successfully')"
  echo "=== DIAGNOSTIC: FLASHINFER backend ==="
  echo "VLLM_ATTENTION_BACKEND=$VLLM_ATTENTION_BACKEND"
  echo "=== DIAGNOSTIC: Complete ==="

  mkdir -p $HOME/data/fleet
  TASKS_FILE="$HOME/data/fleet/tasks_${MODALITY}.json"
  S3_PATH="s3://${S3_DATASET_BUCKET}/${DATA_VERSION}/openenv/all_${MODALITY}.json"
  echo "Downloading dataset from $S3_PATH..."
  aws s3 cp "$S3_PATH" "$TASKS_FILE"
  TASK_COUNT=$(python3 -c "import json; print(len(json.load(open('$TASKS_FILE'))['tasks']))")
  echo "Downloaded $TASK_COUNT tasks for modality: $MODALITY"

  DATA_DIR="$HOME/data/fleet/${MODALITY}"
  PREPARE_CMD="python -m integrations.fleet.prepare_dataset --tasks-json $TASKS_FILE --output-dir $DATA_DIR --modality $MODALITY"
  [ -n "$ENV_KEYS" ] && PREPARE_CMD="$PREPARE_CMD --env-filter $ENV_KEYS"
  eval "$PREPARE_CMD"

run: |
  set -euo pipefail
  cd skyrl-train
  source .venv/bin/activate

  TMP_DIR="$HOME/skyrl-tmp"
  mkdir -p "$TMP_DIR"
  export TMPDIR="$TMP_DIR"

  export PYTORCH_ALLOC_CONF=expandable_segments:True

  TASKS_FILE="$HOME/data/fleet/tasks_${MODALITY}.json"
  DATA_DIR="$HOME/data/fleet/${MODALITY}"

  # Don't use 'uv run' - it re-syncs lockfile and downgrades PyTorch
  python3 -c "import wandb; wandb.login(relogin=True, key='$WANDB_API_KEY')"

  # Diagnostics: verify environment at run time
  echo "=== RUN DIAGNOSTIC: Package versions ==="
  python3 -c "import torch; print(f'torch: {torch.__version__}')"
  python3 -c "import transformers; print(f'transformers: {transformers.__version__}')"
  python3 -c "import vllm; print(f'vllm: {vllm.__version__}')"
  python3 -c "import vllm._C; print('vllm._C imported successfully')"

  # Don't run 'ray stop' - it kills SkyPilot's Ray cluster that manages this job
  export RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook
  export RAY_object_store_memory=10000000000
  ray start --head --disable-usage-stats --port 6479 --object-store-memory=10000000000

  for i in $(seq 1 24); do
    if ray status --address 127.0.0.1:6479 >/dev/null 2>&1; then
      break
    fi
    sleep 5
  done

  TOTAL_GPUS=$SKYPILOT_NUM_GPUS_PER_NODE

  # Diagnostics: test vLLM import in Ray worker
  echo "=== RUN DIAGNOSTIC: Testing vLLM import in Ray worker ==="
  python3 -c "import ray; print(ray.get(ray.remote(lambda: __import__('vllm').__version__).remote()))"
  echo "=== RUN DIAGNOSTIC: Ray worker vLLM import successful ==="

  # Run GRPO training with GLM-4.7-Flash
  # MoE model: 30B total, 3B active - efficient training
  # All deps installed in setup - use python directly (no --isolated)
  python -m integrations.fleet.entrypoints.main_fleet \
    data.train_data="['${DATA_DIR}/train.parquet']" \
    data.val_data="['${DATA_DIR}/validation.parquet']" \
    environment.env_class=fleet_task \
    environment.skyrl_gym.fleet_task.tasks_file="$TASKS_FILE" \
    trainer.algorithm.advantage_estimator="grpo" \
    trainer.policy.model.path="zai-org/GLM-4.7-Flash" \
    trainer.placement.colocate_all=true \
    trainer.strategy=fsdp2 \
    trainer.placement.policy_num_gpus_per_node=$TOTAL_GPUS \
    trainer.placement.ref_num_gpus_per_node=$TOTAL_GPUS \
    generator.num_inference_engines=$TOTAL_GPUS \
    generator.inference_engine_tensor_parallel_size=1 \
    trainer.epochs=${NUM_EPOCHS} \
    trainer.eval_batch_size=24 \
    trainer.eval_before_train=true \
    trainer.eval_interval=20 \
    trainer.update_epochs_per_batch=1 \
    trainer.train_batch_size=24 \
    trainer.use_hybrid_env_sampling=true \
    trainer.min_samples_per_env=1 \
    trainer.policy_mini_batch_size=24 \
    trainer.micro_forward_batch_size_per_gpu=1 \
    trainer.micro_train_batch_size_per_gpu=1 \
    trainer.ckpt_interval=10 \
    trainer.max_ckpts_to_keep=2 \
    trainer.max_prompt_length=512 \
    generator.max_input_length=$MAX_INPUT_LENGTH \
    generator.sampling_params.max_generate_length=$MAX_GENERATE_LENGTH \
    generator.sampling_params.temperature=0.9 \
    generator.sampling_params.top_p=0.95 \
    generator.sampling_params.stop='["</tool_call>"]' \
    generator.eval_sampling_params.stop='["</tool_call>"]' \
    trainer.policy.optimizer_config.lr=1.0e-6 \
    trainer.algorithm.use_kl_loss=true \
    generator.max_turns=$MAX_TURNS \
    generator.backend=$INFERENCE_BACKEND \
    generator.run_engines_locally=true \
    generator.weight_sync_backend=nccl \
    generator.async_engine=true \
    generator.batched=false \
    generator.use_conversation_multi_turn=true \
    generator.n_samples_per_prompt=4 \
    generator.eval_n_samples_per_prompt=3 \
    generator.gpu_memory_utilization=0.7 \
    +generator.reasoning_parser=glm45 \
    +generator.tool_call_parser=glm47 \
    trainer.logger="$LOGGER" \
    trainer.project_name="fleet-task-grpo" \
    trainer.run_name="fleet_${MODALITY}_glm47flash_${RUN_ID:-$(head -c 4 /dev/urandom | xxd -p)}" \
    trainer.resume_mode=latest \
    trainer.ckpt_path="$HOME/ckpts/fleet_${MODALITY}_glm47flash" \
    trainer.dump_data_batch=true
