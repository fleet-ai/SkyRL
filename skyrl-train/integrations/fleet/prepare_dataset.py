"""
Prepare Fleet tasks for SkyRL training.

Converts Fleet task JSON files to SkyRL parquet dataset format.

Usage:
    python -m integrations.fleet.prepare_dataset \
        --tasks-json /path/to/all_tool_use.json \
        --output-dir ./data/fleet \
        --modality tool_use

Split Strategy:
    - Stratified by environment (each env maintains train/eval ratio)
    - Hash-based deterministic assignment (same task always goes to same split)
    - 10% eval ratio, capped at 30 samples per env (MAX_EVAL_SAMPLES)
    - Minimum 1 eval sample per env (otherwise all go to train)
    - Held-out eval envs: instacart (computer_use only)

v0.3.1 Changes:
    - Added MAX_ENV_TRAIN_RATIO=0.20 to prevent any single env from dominating
    - Hash-based deterministic sampling for reproducibility

v0.3.0 Changes:
    - Increased eval_ratio from 2% to 10%
    - Added MAX_EVAL_SAMPLES=30 cap per environment
    - MIN_EVAL_SAMPLES stays at 5
    - Result: ticketmaster now gets ~22 eval samples for trace analysis
"""

import argparse
import hashlib
import json
import os
from collections import defaultdict
from typing import Any, Dict, List, Optional

from datasets import Dataset

# Held-out environments for eval only (not used in train)
HELD_OUT_ENVS = {
    "tool_use": [],  # v0.3: all envs split normally (outlook now included in train)
    "computer_use": ["instacart"],
}

# Minimum number of samples required to create an eval split for an env
MIN_EVAL_SAMPLES = 5

# Maximum number of eval samples per environment (v0.3.1: reduced from 30 to 20)
# Ensures small envs get eval traces without blowing up eval set size
MAX_EVAL_SAMPLES = 20

# Maximum total eval prompts (v0.3.2: cap total eval set size for faster evals)
MAX_EVAL_PROMPTS = 50

# Maximum fraction of training data any single environment can have (v0.3.1)
# Prevents dominant environments from skewing training
MAX_ENV_TRAIN_RATIO = 0.20


def load_tasks_from_json(json_path: str) -> List[Dict[str, Any]]:
    """Load tasks from JSON file (Fleet export format)."""
    with open(json_path, "r") as f:
        data = json.load(f)

    # Handle both formats: array or {"tasks": [...]}
    if isinstance(data, list):
        return data
    elif isinstance(data, dict) and "tasks" in data:
        return data["tasks"]
    else:
        raise ValueError("Invalid JSON format: expected array or object with 'tasks' key")


def hash_to_split(task_key: str, eval_ratio: float = 0.10) -> str:
    """Deterministically assign task to train or eval based on hash.

    Uses MD5 hash of task_key to get a deterministic float in [0, 1).
    This ensures the same task always goes to the same split.
    """
    hash_bytes = hashlib.md5(task_key.encode()).digest()
    hash_int = int.from_bytes(hash_bytes[:8], byteorder="big")
    hash_float = hash_int / (2**64)
    return "eval" if hash_float < eval_ratio else "train"


def hash_to_float(task_key: str) -> float:
    """Convert task_key to deterministic float in [0, 1) for sampling."""
    hash_bytes = hashlib.md5(task_key.encode()).digest()
    hash_int = int.from_bytes(hash_bytes[:8], byteorder="big")
    return hash_int / (2**64)


def cap_training_distribution(
    train_records: List[Dict[str, Any]],
    max_env_ratio: float,
) -> tuple[List[Dict[str, Any]], Dict[str, Dict[str, int]]]:
    """Cap each environment's contribution to training data.

    Uses hash-based deterministic sampling so the same tasks are always selected.

    Args:
        train_records: List of training records with 'data_source' (env_key) and 'task_key'
        max_env_ratio: Maximum fraction any single env can contribute (e.g., 0.20 = 20%)

    Returns:
        Tuple of (capped_records, cap_stats) where cap_stats shows per-env before/after counts
    """
    if max_env_ratio >= 1.0:
        return train_records, {}

    total_train = len(train_records)
    max_per_env = int(total_train * max_env_ratio)

    # Group by environment
    records_by_env: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for record in train_records:
        env_key = record.get("data_source", "unknown")
        records_by_env[env_key].append(record)

    # Cap each environment
    capped_records = []
    cap_stats: Dict[str, Dict[str, int]] = {}

    for env_key, records in records_by_env.items():
        before_count = len(records)

        if before_count <= max_per_env:
            # No capping needed
            capped_records.extend(records)
            cap_stats[env_key] = {"before": before_count, "after": before_count, "capped": False}
        else:
            # Sort by hash for deterministic selection
            records_sorted = sorted(records, key=lambda r: hash_to_float(r.get("task_key", "")))
            selected = records_sorted[:max_per_env]
            capped_records.extend(selected)
            cap_stats[env_key] = {"before": before_count, "after": max_per_env, "capped": True}

    return capped_records, cap_stats


def prepare_fleet_dataset(
    tasks_json: str,
    output_dir: str,
    modality: Optional[str] = "tool_use",
    eval_ratio: float = 0.10,  # v0.3: increased from 0.02 to ensure ticketmaster gets eval samples
    env_filter: Optional[str] = None,
    max_tasks: Optional[int] = None,
    max_env_ratio: float = MAX_ENV_TRAIN_RATIO,  # v0.3.1: cap dominant environments
    max_eval_prompts: int = MAX_EVAL_PROMPTS,  # v0.3.2: cap total eval set size
):
    """
    Convert Fleet tasks JSON to SkyRL parquet dataset.

    Args:
        tasks_json: Path to Fleet tasks JSON file
        output_dir: Output directory for parquet files
        modality: Task modality filter ("tool_use" or "computer_use"), None for all
        eval_ratio: Fraction of data for evaluation (default: 0.02)
        env_filter: Optional env_key filter (e.g., "github", "booking")
        max_tasks: Optional maximum number of tasks to include
        max_env_ratio: Maximum fraction any single env can contribute to training (default: 0.20)
    """
    print(f"Loading tasks from {tasks_json}...")
    tasks = load_tasks_from_json(tasks_json)
    print(f"Loaded {len(tasks)} tasks")

    # Filter by modality if specified
    if modality:
        tasks = [t for t in tasks if t.get("task_modality") == modality]
        print(f"After modality filter ({modality}): {len(tasks)} tasks")

    # Filter by env_key if specified
    if env_filter:
        tasks = [t for t in tasks if t.get("env_key") == env_filter or t.get("env_id") == env_filter]
        print(f"After env filter ({env_filter}): {len(tasks)} tasks")

    # Limit tasks if specified
    if max_tasks and len(tasks) > max_tasks:
        tasks = tasks[:max_tasks]
        print(f"Limited to {max_tasks} tasks")

    if not tasks:
        print("No tasks remaining after filtering. Exiting.")
        return

    # Deduplicate by task_key (keep first occurrence)
    seen_task_keys: set = set()
    unique_tasks = []
    duplicate_count = 0
    env_duplicate_counts: Dict[str, int] = defaultdict(int)

    for task in tasks:
        task_key = task.get("key") or task.get("task_key")
        if not task_key:
            continue
        if task_key in seen_task_keys:
            duplicate_count += 1
            env_key = task.get("env_key") or task.get("env_id") or "unknown"
            env_duplicate_counts[env_key] += 1
        else:
            seen_task_keys.add(task_key)
            unique_tasks.append(task)

    if duplicate_count > 0:
        print(f"\n⚠️  WARNING: Removed {duplicate_count} duplicate task_keys")
        print("  By environment:")
        for env, count in sorted(env_duplicate_counts.items(), key=lambda x: -x[1]):
            print(f"    {env}: {count} duplicates removed")
        print()

    tasks = unique_tasks
    print(f"After deduplication: {len(tasks)} unique tasks")

    # Get held-out envs for this modality
    held_out_envs = set(HELD_OUT_ENVS.get(modality, []))
    if held_out_envs:
        print(f"Held-out test environments: {held_out_envs}")

    # Group tasks by environment
    tasks_by_env: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for task in tasks:
        env_key = task.get("env_key") or task.get("env_id") or "unknown"
        tasks_by_env[env_key].append(task)

    # Prepare records with stratified split
    train_records = []
    eval_records = []

    # Track per-env counts for summary table
    env_split_counts: Dict[str, Dict[str, int]] = {}

    print("\n=== Per-Environment Split ===")
    for env_key in sorted(tasks_by_env.keys()):
        env_tasks = tasks_by_env[env_key]

        # Check if this env is held out for eval only
        if env_key in held_out_envs:
            env_eval_count = 0
            for task in env_tasks:
                record = _task_to_record(task, env_key)
                if record:
                    eval_records.append(record)
                    env_eval_count += 1
            env_split_counts[env_key] = {"train": 0, "eval": env_eval_count}
            print(f"  {env_key}: {len(env_tasks)} -> EVAL only (held-out)")
            continue

        # Calculate target eval size: use ratio but cap at MAX_EVAL_SAMPLES
        target_eval_size = min(int(len(env_tasks) * eval_ratio), MAX_EVAL_SAMPLES)

        # If not enough samples for eval, put all in train
        if target_eval_size < MIN_EVAL_SAMPLES:
            env_train_count = 0
            for task in env_tasks:
                record = _task_to_record(task, env_key)
                if record:
                    train_records.append(record)
                    env_train_count += 1
            env_split_counts[env_key] = {"train": env_train_count, "eval": 0}
            print(f"  {env_key}: {len(env_tasks)} -> all TRAIN (< {MIN_EVAL_SAMPLES} eval samples)")
            continue

        # Compute effective eval ratio to achieve target_eval_size (capped at MAX_EVAL_SAMPLES)
        effective_eval_ratio = target_eval_size / len(env_tasks)

        # Stratified split using hash with effective ratio
        env_train = 0
        env_eval = 0
        for task in env_tasks:
            task_key = task.get("key") or task.get("task_key")
            record = _task_to_record(task, env_key)
            if not record:
                continue

            split = hash_to_split(task_key, effective_eval_ratio)
            if split == "eval":
                eval_records.append(record)
                env_eval += 1
            else:
                train_records.append(record)
                env_train += 1

        env_split_counts[env_key] = {"train": env_train, "eval": env_eval}
        print(f"  {env_key}: {len(env_tasks)} -> {env_train} train, {env_eval} eval")

    print(f"\nTotal: {len(train_records)} train, {len(eval_records)} eval")

    # Apply total eval cap (v0.3.2) - sample stratified by environment
    if max_eval_prompts and len(eval_records) > max_eval_prompts:
        # Group by environment for stratified sampling
        eval_by_env: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for record in eval_records:
            env_key = record.get("data_source", "unknown")
            eval_by_env[env_key].append(record)

        # Calculate per-env quota (proportional to original size)
        total_eval = len(eval_records)
        capped_eval_records = []
        for env_key, records in eval_by_env.items():
            quota = max(1, int(len(records) / total_eval * max_eval_prompts))
            # Sort by hash for deterministic selection
            records_sorted = sorted(records, key=lambda r: hash_to_float(r.get("task_key", "")))
            capped_eval_records.extend(records_sorted[:quota])

        print(f"\n=== Eval Set Cap ({max_eval_prompts} max prompts) ===")
        print(f"  {len(eval_records)} -> {len(capped_eval_records)} eval prompts")
        eval_records = capped_eval_records

    # Apply per-environment cap to training data (v0.3.1)
    if max_env_ratio < 1.0 and train_records:
        train_records, cap_stats = cap_training_distribution(train_records, max_env_ratio)

        # Print capping summary
        capped_envs = [env for env, stats in cap_stats.items() if stats["capped"]]
        if capped_envs:
            print(f"\n=== Training Distribution Cap ({max_env_ratio:.0%} max per env) ===")
            for env in sorted(capped_envs):
                stats = cap_stats[env]
                print(f"  {env}: {stats['before']} -> {stats['after']} ({stats['before'] - stats['after']} removed)")
            print(f"\nAfter capping: {len(train_records)} train")

        # Update env_split_counts with capped values
        for env, stats in cap_stats.items():
            if env in env_split_counts:
                env_split_counts[env]["train"] = stats["after"]

    # Create datasets
    train_dataset = Dataset.from_list(train_records) if train_records else None
    eval_dataset = Dataset.from_list(eval_records) if eval_records else None

    # Save to parquet
    os.makedirs(output_dir, exist_ok=True)

    if train_dataset:
        train_path = os.path.join(output_dir, "train.parquet")
        train_dataset.to_parquet(train_path)
        print(f"Saved train dataset to {train_path}")

    if eval_dataset:
        eval_path = os.path.join(output_dir, "validation.parquet")
        eval_dataset.to_parquet(eval_path)
        print(f"Saved validation dataset to {eval_path}")

    # Print summary statistics
    print("\n=== Dataset Summary ===")
    print(f"Train: {len(train_records)}")
    print(f"Eval:  {len(eval_records)} (includes held-out: {held_out_envs or 'none'})")

    # Print per-environment breakdown table
    print("\n=== Per-Environment Breakdown ===")
    print(f"{'Environment':<20} {'Train':>8} {'Eval':>8} {'Total':>8}")
    print("-" * 48)
    for env_key in sorted(env_split_counts.keys()):
        counts = env_split_counts[env_key]
        total = counts["train"] + counts["eval"]
        print(f"{env_key:<20} {counts['train']:>8} {counts['eval']:>8} {total:>8}")
    print("-" * 48)
    print(
        f"{'TOTAL':<20} {len(train_records):>8} {len(eval_records):>8} " f"{len(train_records) + len(eval_records):>8}"
    )


def _task_to_record(task: Dict[str, Any], env_key: str) -> Optional[Dict[str, Any]]:
    """Convert a task dict to a dataset record."""
    task_key = task.get("key") or task.get("task_key")
    prompt = task.get("prompt", "")

    if not task_key or not prompt:
        return None

    return {
        # Required fields for SkyRL
        "prompt": [{"role": "user", "content": prompt}],
        "env_class": "fleet_task",  # This tells SkyRL to use FleetTaskEnv
        # Task identification (passed as env_extras)
        "task_key": task_key,
        # Data source for per-environment metrics in WandB
        "data_source": env_key,
    }


def main():
    parser = argparse.ArgumentParser(description="Prepare Fleet tasks for SkyRL training")
    parser.add_argument(
        "--tasks-json",
        type=str,
        required=True,
        help="Path to Fleet tasks JSON file",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./data/fleet",
        help="Output directory for parquet files",
    )
    parser.add_argument(
        "--modality",
        type=str,
        default="tool_use",
        choices=["tool_use", "computer_use", "all"],
        help="Task modality filter ('all' for no filter)",
    )
    parser.add_argument(
        "--eval-ratio",
        type=float,
        default=0.10,
        help="Fraction of data for evaluation (default: 0.10)",
    )
    parser.add_argument(
        "--env-filter",
        type=str,
        default=None,
        help="Optional env_key filter (e.g., 'github', 'booking')",
    )
    parser.add_argument(
        "--max-tasks",
        type=int,
        default=None,
        help="Maximum number of tasks to include",
    )
    parser.add_argument(
        "--max-env-ratio",
        type=float,
        default=MAX_ENV_TRAIN_RATIO,
        help=f"Maximum fraction of training data per environment (default: {MAX_ENV_TRAIN_RATIO})",
    )
    parser.add_argument(
        "--max-eval-prompts",
        type=int,
        default=MAX_EVAL_PROMPTS,
        help=f"Maximum total eval prompts (default: {MAX_EVAL_PROMPTS})",
    )

    args = parser.parse_args()

    # Handle 'all' modality
    modality = None if args.modality == "all" else args.modality

    prepare_fleet_dataset(
        tasks_json=args.tasks_json,
        output_dir=args.output_dir,
        modality=modality,
        eval_ratio=args.eval_ratio,
        env_filter=args.env_filter,
        max_tasks=args.max_tasks,
        max_env_ratio=args.max_env_ratio,
        max_eval_prompts=args.max_eval_prompts,
    )


if __name__ == "__main__":
    main()
