"""
SWE Task environment utilities for SkyRL integration.

Mirrors the pattern from examples/mini_swe_agent/mini_swe_utils.py but
adapted for our swe-task-generator task format. Key differences:
  - image_name comes directly from task.json (already on Docker Hub)
  - eval_script is stored in the instance metadata
  - No SWE-Bench specific image naming conventions needed
"""

import uuid
import traceback
from typing import TypedDict, Optional, Dict, Any

from loguru import logger
from jinja2 import Template
from minisweagent.environments import Environment, get_environment


class SWETaskEvaluationResult(TypedDict):
    instance_id: str
    resolved: bool
    eval_error: Optional[str]


def get_swe_task_environment(config: dict, instance: dict) -> Environment:
    """
    Create a Docker environment for a SWE task instance.

    Args:
        config: Environment configuration (from swe_tasks.yaml)
        instance: Task instance dict with 'image_name' field

    Returns:
        Environment: A minisweagent Docker environment ready for agent interaction
    """
    env_config = config.setdefault("environment", {})
    env_config["environment_class"] = env_config.get("environment_class", "docker")

    image_name = instance["image_name"]

    if env_config["environment_class"] == "docker":
        env_config["image"] = image_name
    elif env_config["environment_class"] == "singularity":
        env_config["image"] = f"docker://{image_name}"

    env = get_environment(env_config)

    if startup_command := config.get("run", {}).get("env_startup_command"):
        startup_command = Template(startup_command).render(**instance)
        out = env.execute(startup_command)
        if out["returncode"] != 0:
            raise RuntimeError(f"Error executing startup command: {out}")

    return env


def evaluate_trajectory(instance: Dict[str, Any], model_patch: str, config: dict) -> SWETaskEvaluationResult:
    """
    Evaluate a model-generated patch against the task's eval_script.

    This is called after the agent finishes its trajectory. It:
    1. Creates a fresh Docker environment
    2. Applies the model's git patch
    3. Runs the eval_script from the task instance
    4. Returns whether the task was resolved

    Args:
        instance: Task instance dict with 'instance_id', 'eval_script', 'image_name'
        model_patch: The git diff patch generated by the model
        config: Environment configuration

    Returns:
        SWETaskEvaluationResult with resolved=True/False
    """
    ret = SWETaskEvaluationResult(
        instance_id=instance["instance_id"],
        resolved=False,
        eval_error=None,
    )

    env = None
    try:
        env = get_swe_task_environment(config, instance)
    except Exception as e:
        ret["eval_error"] = f"Env creation failed: {e}"
        logger.info(f"Environment creation failed: {e}\n{traceback.format_exc()}")
        return ret

    # Apply model's git patch using heredoc to avoid ARG_MAX issues
    delimiter = f"PATCH_{uuid.uuid4().hex}"
    command = f"cd /workspace && git apply <<'{delimiter}'\n{model_patch}\n{delimiter}"

    obs = env.execute(command)

    if obs["returncode"] != 0:
        ret["eval_error"] = obs["output"]
    else:
        # Run eval script from the task instance
        eval_script = instance["eval_script"]
        eval_cmd = f"bash <<'EOF'\n{eval_script}\nEOF"
        obs = env.execute(eval_cmd, timeout=3600)
        ret["resolved"] = obs["returncode"] == 0
        ret["eval_error"] = f"(truncated to last 1000 chars)\n{obs['output'][-1000:]}" if not ret["resolved"] else None

    return ret
